{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TNE 2: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to use Principal Component Analysis (PCA) \n",
    "for dimension reduction applied to images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import linalg as la\n",
    "from numpy.testing import assert_array_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Application: handwritten digits recognition 5 & 6\n",
    "We load 2 matrices which contain each a sequence of examples of 16x16 images of handwritten digits which are 5 and 6 here. Each line of the matrix contains 256 pixel values coding for the gray level of a 16x16 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_5 = np.loadtxt('train_5.txt',delimiter=',')   # 556 samples\n",
    "train_6 = np.loadtxt('train_6.txt',delimiter=',')   # 664 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 5\n",
    "n=7\n",
    "I = np.reshape(train_5[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 6\n",
    "n=2\n",
    "I = reshape(train_6[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating the training and test sets\n",
    "\n",
    "We keep in the training set the 145 first images of 5s and the 200 first\n",
    "images of 6s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_brut = np.vstack((train_5[:145,:], train_6[:200,:]))\n",
    "N_train = np.size(x_train_brut,axis=0)\n",
    "class_train = np.ones((345,1))   # label 1 for digit 6\n",
    "class_train[:145] = 0       # label 0 for digit 5\n",
    "\n",
    "x_test_brut = np.vstack((train_5[145:,:], train_6[200:,:]))\n",
    "N_test = np.size(train_5,axis=0)+np.size(train_6,axis=0)-N_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis\n",
    "\n",
    "The purpose of this part is to observe the respective contributions of\n",
    "each component of a PCA of images of 5. The function `sklearn.decomposition.PCA` of `scikit-learn` is available. In practice, one must first estimate the mean vector and then work with centered data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "First have a look at\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal component analysis\n",
    "moy_train = x_train_brut.mean(axis=0)  # all the data, 5 & 6\n",
    "x_train_centre = x_train_brut-np.tile(moy_train,(N_train,1))\n",
    "\n",
    "# PCA from scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 250\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(x_train_centre)  # you may forget centering that is done by sklearn PCA\n",
    "\n",
    "singval = pca.singular_values_   # eigenvalues\n",
    "comp = pca.components_           # principal components\n",
    "proj = pca.transform(x_train_centre)  # computes the projection coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the averaged images of 5 & 6 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_moy = np.reshape(moy_train,(16,16))   # averaged image = mean \n",
    "plt.imshow(I_moy,cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display an example rebuilt from the 1st component only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=7   # choice of image no n=7 or any other\n",
    "I = I_moy + proj[n,0]*np.reshape(comp[0,:],(16,16)) # adding the 1st PCA component\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: PCA & approximation\n",
    "\n",
    "1. Read the documentation of function `PCA` and identify the input and output parameters.\n",
    "2. Implement a progressive reconstruction of an image of digit 5 by adding the successive \n",
    "contribution of principal components.\n",
    "3. Observe graphical results. How many components are necessary to obtain a \n",
    "reconstruction that you may consider as acceptable? nice? very nice?\n",
    "4. Optional question: do the same for 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 1</u> : Description of the PCA function :\n",
    "- Inputs parameters:    \n",
    "  - n_components = Number of components to keep. Defaults to all components\n",
    "  - copy = If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead\n",
    "  - whiten = the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.\n",
    "  - svd_solver = Solver in {'auto', 'full', 'arpack', 'randomized'}\n",
    "  - tol = Tolerance for singular values computed by svd_solver == ‘arpack’. Must be of range [0.0, infinity[\n",
    "  - iterated_power = Number of iterations for the power method computed by svd_solver == ‘randomized’. Must be of range [0, infinity[.\n",
    "  - n_oversamples = This parameter is only relevant when svd_solver=\"randomized\". It corresponds to the additional number of random vectors to sample the range of X so as to ensure proper conditioning\n",
    "  - power_iteration_normalizer = Power iteration normalizer for randomized SVD solver. Not used by ARPACK. See randomized_svd for more details.\n",
    "  - random_state = Used when the ‘arpack’ or ‘randomized’ solvers are used. Pass an int for reproducible results across multiple function calls\n",
    "- Output attributes :\n",
    "  - components_ = Principal axes in feature space, representing the directions of maximum variance in the data. Equivalently, the right singular vectors of the centered input data, parallel to its eigenvectors. The components are sorted by explained_variance_\n",
    "  - explained_variance_ = The amount of variance explained by each of the selected components. The variance estimation uses n_samples - 1 degrees of freedom. Equal to n_components largest eigenvalues of the covariance matrix of X\n",
    "  - explained_variance_ratio_ = Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\n",
    "  - singular_values_ = The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components variables in the lower-dimensional space\n",
    "  - mean_ = Per-feature empirical mean, estimated from the training set. Equal to X.mean(axis=0).\n",
    "  - n_components_ = The estimated number of components. When n_components is set to ‘mle’ or a number between 0 and 1 (with svd_solver == ‘full’) this number is estimated from input data. Otherwise it equals the parameter n_components, or the lesser value of n_features and n_samples if n_components is None\n",
    "  - n_features_ = Number of features in the training data\n",
    "  - n_samples_ = Number of samples in the training data\n",
    "  - noise_variance_ = The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999\n",
    "  - n_features_in_ = Number of features seen during fit\n",
    "  - feature_names_in_ = Names of features seen during fit. Defined only when X has feature names that are all strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 2</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_reconstruction(n, proj, comp):\n",
    "    \"\"\"Progressive reconstruction of an image by adding successive contributions of the PCA\n",
    "\n",
    "    Args:\n",
    "        n (int): Input image\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: image reconstructed \n",
    "    \"\"\"\n",
    "    I = I_moy + np.reshape(proj[n,:] @ comp,(16,16))\n",
    "    return I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 3</u> : Graphically, we can say that the reconstruction is :\n",
    "- ACCEPTABLE : 20 to 50 components\n",
    "- NICE : 50 to 150\n",
    "- VERY NICE : 150 to All components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "n_components = [5,10,20,30,50,100,150,200,250]\n",
    "fig, ax = plt.subplot_mosaic([[0,1,2],\n",
    "                              [3,4,5],\n",
    "                              [6,7,8]], figsize=(16,16))\n",
    "\n",
    "for inc in range(len(n_components)):\n",
    "    pca = PCA(n_components = n_components[inc])\n",
    "    pca.fit(x_train_centre)\n",
    "    comp = pca.components_\n",
    "    proj = pca.transform(x_train_centre)\n",
    "    \n",
    "    I_reconstructed = progressive_reconstruction(n, proj, comp)\n",
    "    ax[inc].imshow(I_reconstructed, cmap=\"gray\")\n",
    "    ax[inc].set_title(f\"Reconstruction with {n_components[inc]} components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 4</u> : For the 6 images, we can accept the following results :\n",
    "- ACCEPTABLE : 10 to 50 components\n",
    "- NICE : 50 to 150\n",
    "- VERY NICE : 150 to All components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "n_components = [5,10,20,30,50,100,150,200,250]\n",
    "fig, ax = plt.subplot_mosaic([[0,1,2],\n",
    "                              [3,4,5],\n",
    "                              [6,7,8]], figsize=(16,16))\n",
    "\n",
    "for inc in range(len(n_components)):\n",
    "    pca = PCA(n_components = n_components[inc])\n",
    "    pca.fit(x_train_centre)\n",
    "    comp = pca.components_\n",
    "    proj = pca.transform(x_train_centre)\n",
    "    \n",
    "    I_reconstructed = progressive_reconstruction(n, proj, comp)\n",
    "    ax[inc].imshow(I_reconstructed, cmap=\"gray\")\n",
    "    ax[inc].set_title(f\"Reconstruction with {n_components[inc]} components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: PCA & classification\n",
    "1. Use `proj[0:2,:]` as the coordinates of a point representing each sample\n",
    "of the training set in a plane. Display the cloud of points associated to\n",
    "digits 5 and 6 by using 2 different colors.\n",
    "2. Comment on the repartition of points in the plane. \n",
    "3. Do you see how this PCA step makes possible the use of a much simpler classification? \n",
    "What would you propose as an alternative to logistic regression of TP3 then?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 1</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_5 = [proj[i,0] for i in range(145)]\n",
    "x_6 = [proj[i,0] for i in range(145,345)]\n",
    "y_5 = [proj[i,1] for i in range(145)]\n",
    "y_6 = [proj[i,1] for i in range(145,345)]\n",
    "plt.scatter(x_5, y_5, label=\"Images 5\")\n",
    "plt.scatter(x_6, y_6, label=\"Images 6\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")\n",
    "plt.title(\"PCA of Images dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 2</u> : We can clearly identify two clusters for images 5 and 6 only with the plotting along the 2 first principal components. Quantitatively, we can compute the explained variance for the two first components :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First component : {pca.explained_variance_ratio_[0]}\")\n",
    "print(f\"Second component : {pca.explained_variance_ratio_[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Answer 3</u> : PCA is a dimension reduction tool, not a classifier. In Scikit-Learn, all classifiers and estimators have a predict method which PCA does not. Once need to fit a classifier on the PCA-transformed data. Scikit-Learn has many classifiers such as the LDA and QDA seen in TP3.\n",
    "An alternative to TP3 would be to use PCA-transformed data with any classifier in sklearn and there are many :\n",
    "- Nearest Neighbors\n",
    "- Linear SVM\n",
    "- RBF SVM\n",
    "- Gaussian Process\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Neural Net\n",
    "- AdaBoost\n",
    "- Naive Bayes\n",
    "- QDA/LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA/LDA :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    \"\"\"This class implement the linear discriminant analysis specifically for the loaded dataset synth. This class need to be loaded with a train and test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, train, test):\n",
    "        self.train_df = pd.DataFrame(train,columns = ['classe', 'x1', 'x2'])\n",
    "        self.test_df = pd.DataFrame(test,columns = ['classe', 'x1', 'x2'])\n",
    "        self.type = \"LDA\"\n",
    "        \n",
    "        \n",
    "    def get_pi_estimators(self)->list:\n",
    "        \"\"\"Returns the pi estimators for each class.\n",
    "\n",
    "        Returns:\n",
    "            list: list of floats\n",
    "        \"\"\"\n",
    "        return [pi for pi in self.train_df.classe.value_counts(normalize=True, ascending=True).values]\n",
    "    \n",
    "    def get_mu_estimators(self)->np.ndarray:\n",
    "        \"\"\"Returns the mu estimators for each class.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: an array where each line returns the vector mu (estimator) for the concerned class\n",
    "        \"\"\"\n",
    "        classes = [1,2]\n",
    "        mu = np.zeros((len(classes), 2))\n",
    "        for i, c in enumerate(classes):\n",
    "            mu[i] = self.train_df[self.train_df.classe==c][['x1', 'x2']].sum(axis=0).to_numpy() / self.train_df[self.train_df.classe==c].shape[0]\n",
    "        return mu\n",
    "    \n",
    "    def get_sigma_estimators(self)->np.ndarray:\n",
    "        \"\"\"Returns the average sigma estimator.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Sigma array in dimension 2x2\n",
    "        \"\"\"\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = [1,2]\n",
    "        sigma_moy = np.zeros((2,2))\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_moy += sigma\n",
    "        \n",
    "        return sigma_moy/self.train_df.shape[0]\n",
    "    \n",
    "    def get_log_probabilities(self, df:pd.DataFrame)->np.ndarray:\n",
    "        \"\"\"Compute the log_probability for the entries.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with the columns x1 and x2\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matrix in dim Nx2 where N is the shape of entries\n",
    "        \"\"\"\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) + x.T @ la.inv(sigma) @ mu[j].reshape((2,1)) - 1/2 * mu[j].reshape((2,1)).T @ la.inv(sigma) @ mu[j].reshape((2,1))\n",
    "            prediction[i] = y\n",
    "        return prediction\n",
    "    \n",
    "    def classification(self, train=True)->np.ndarray:\n",
    "        \"\"\"Returns the classification using discriminant analysis.\n",
    "\n",
    "        Args:\n",
    "            train (bool, optional): Use the trainset if True and the testset if not. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Vector with class id for each entry.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            df = self.train_df\n",
    "        else:\n",
    "            df = self.test_df\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        return np.argmax(prediction, axis=1) + 1\n",
    "    \n",
    "    def error_rate(self, train=True)->float:\n",
    "        classes = self.classification(train=train)\n",
    "        if train:\n",
    "            results = (classes == self.train_df.classe.to_numpy())\n",
    "            error_rate = 1 - np.count_nonzero(results)/self.train_df.shape[0]\n",
    "        else:\n",
    "            results = (classes == self.test_df.classe.to_numpy())\n",
    "            error_rate = 1 - np.count_nonzero(results)/self.test_df.shape[0]\n",
    "        return error_rate\n",
    "        \n",
    "    \n",
    "    def plot_decision_boundary(self):\n",
    "        \"\"\"Plot the decision boundary\n",
    "        \"\"\"\n",
    "        Nx1=100 # number of samples for display\n",
    "        Nx2=100\n",
    "        x1=np.linspace(-6,6,Nx1)  # sampling of the x1 axis \n",
    "        x2=np.linspace(-8,8,Nx2)  # sampling of the x2 axis\n",
    "        [X1,X2]=np.meshgrid(x1,x2)  \n",
    "        df = pd.DataFrame({'x1': X1.flatten('F'), 'x2': X2.flatten('F')})\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        classe = list(np.argmax(prediction, axis=1) + 1)\n",
    "        df['classe'] = [f'classe {i}' for i in classe]\n",
    "        fig = px.scatter(df, x=\"x1\", y=\"x2\", color = \"classe\", title=f\"Decision boundary with {self.type}\")\n",
    "        fig.write_image(f\"figures/decision_boundary_with_{self.type}.png\")\n",
    "        fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA(LDA):\n",
    "    \"\"\"This class implement the linear discriminant analysis specifically for the loaded dataset synth. This class need to be loaded with a train and test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, train, test):\n",
    "        super().__init__(train, test)\n",
    "        self.type = \"QDA\"\n",
    "    \n",
    "    def get_sigma_estimators(self)->list:\n",
    "        \"\"\"Returns the sigma estimator for each class.\n",
    "\n",
    "        Returns:\n",
    "            list: List of matrix in dim 2x2\n",
    "        \"\"\"\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = [1,2]\n",
    "        sigma_list =[]\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_list.append(sigma/train_df_c.shape[0])\n",
    "        \n",
    "        return sigma_list\n",
    "    \n",
    "    def get_log_probabilities(self, df):\n",
    "        \"\"\"Compute the log_probability for the entries.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with the columns x1 and x2\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matrix in dim Nx2 where N is the shape of entries\n",
    "        \"\"\"\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) - 1/2 * np.log(la.det(sigma[j])) -1/2 *  (x - mu[j].reshape((2,1))).T @ la.inv(sigma[j]) @ (x - mu[j].reshape((2,1)))\n",
    "            prediction[i] = y\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([1 for _ in range(145)] + [2 for _ in range(200)]).reshape(-1,1)\n",
    "proj_train = np.c_[y_train,proj[:, 0:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_centre = x_test_brut-np.tile(moy_train,(N_test,1))\n",
    "y_test = np.array([1 for _ in range(411)] + [2 for _ in range(464)]).reshape(-1,1)\n",
    "proj_test = np.c_[y_test,pca.transform(x_test_centre)[:, 0:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_classifier = LDA(proj_train, proj_test)\n",
    "qda_classifier = QDA(proj_train,proj_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_classifier.error_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_classifier.error_rate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a very good error rate. In our case, the QDA performs better than the LDA, let's look at the confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = qda_classifier.classification(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(proj_test[:, 0], preds, labels = [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a confusion matrix as good as in the previous tp by using only two components. In addition, we used much less data for training than in the previous tp which shows the robustness of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbors : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [i for i in range(2,20)]\n",
    "n_best = -1\n",
    "best_error_rate = 1.\n",
    "for n in n_neighbors:\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors = n)\n",
    "    knn_classifier.fit(proj[:,0:2], y_train.reshape(1,-1)[0])\n",
    "    error_rate = 1 - knn_classifier.score(proj_test[:, 1:3], y_test.reshape(1,-1)[0])\n",
    "    if error_rate<best_error_rate:\n",
    "        best_error_rate=error_rate\n",
    "        n_best = n\n",
    "        best_knn = knn_classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best value for n_neighbors is : {n_best}.\", f\"The error rate associated is {best_error_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(best_knn, proj_test[:, 1:3], y_test.reshape(1,-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA + KNN is better than PCA + QDA/LDA which could be predicted from the decision frontier using the first two components of the cpd. Indeed, it is impossible to construct a perfect quadratic or linear decision frontier from the data used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that using PCA in advance of a classification is a very good way to retrieve only the important information from an image (works with any type of input) that allows the classification. To go further, we could do a Fisher dimension reduction coupled with one of the algorithms mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyong this lab\n",
    "\n",
    "Have a look at other examples of applications, like\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
