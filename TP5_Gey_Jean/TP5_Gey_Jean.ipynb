{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP5: Decision trees & random forests\n",
    "The aim of this tutorial is to get familiar with the use of decision trees and their generalizations on simple examples using `scikit-learn` tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing your installation first\n",
    "You will need to install packages `python-graphviz` first. If needed, uncomment the `conda` command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If needed, uncomment the line below:\n",
    "# pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import graphviz \n",
    "import pandas as pd\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this tutorial is famous. Called, **the iris dataset**, it contains four variables measuring various parts of iris flowers of three related species, and then a fourth variable with the species name. The reason it is so famous in machine learning and statistics communities is because the data requires very little preprocessing (i.e. no missing values, all features are floating numbers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: explore the data set\n",
    "1. What is the structure of the object `iris` ?\n",
    "\n",
    "2. Plot this dataset in a well chosen set of representations to explore the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Given that iris is a dictionary, we use the pandas library to tranform it into a DataFrame and explore the data in a well chosen set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `pandas` to manipulate the data\n",
    "Pandas is great to manipulate data in a Microsoft Excel like way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with the species names, this is what we are going to try to predict\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: create training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new column that for each row, generates a random number between 0 and 1, and if that value is less than or equal to .75, then sets the value of that cell as True and false otherwise. This is a quick and dirty way of randomly assigning some rows to be used as the training data and some as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two new dataframes, one with the training rows, one with the test rows\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of observations for the test and training dataframes\n",
    "print('Number of observations in the training data:', len(train))\n",
    "print('Number of observations in the test data:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the feature column's names\n",
    "features = df.columns[:4]\n",
    "\n",
    "# View features\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['species'] contains the actual species names. Before we can use it,\n",
    "# we need to convert each species name into a digit. So, in this case there\n",
    "# are three species, which have been coded as 0, 1, or 2.\n",
    "y = pd.factorize(train['species'])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Decision trees for the iris dataset\n",
    "The method `tree.DecisionTreeClassifier()` from `scikit-learn` builds decision trees objects as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train[features], y)\n",
    "\n",
    "# Using the whole dataset you may use directly:\n",
    "#clf = clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `export_graphviz` exporter supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also export the tree in Graphviz format and  savethe resulting graph in an output file iris.pdf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being fitted, **the model can then be used to predict the class of samples**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pred = clf.predict(iris.data[:1, :])\n",
    "class_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "1. Train the decision tree on the iris dataset and explain how one should read blocks in `graphviz` representation of the tree.\n",
    "\n",
    "2. Plot the regions of decision with the points of the training set superimposed.\n",
    "\n",
    "*Indication: you may find the function `plt.contourf` useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Réponse 1</u> : Pour entraîner l'arbre de décision sur les données du dataset Iris et les plots en utilisant Graphviz, on reprend les codes précédents :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "y = pd.factorize(train['species'])[0]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(train[features], y)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Explication</b> : Un arbre de décision se compose de noeuds (carrés intermédiaires) et de feuilles (carrés en bout de chaîne). Chaque noeud comporte l'information relative à une décision. Si toutes les informations du noeud sont respectées, on passe au noeud enfant suivant la flèche True, sinon on passe au noeud enfant suivant la flèche False. Cela permet en bout de course d'arriver à une feuille qui comporte l'information de la décision prise par l'arbre au final."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Réponse 2</u> : On dessine les frontières de décision (entre chaque paire d'attributs) relative à l'arbre de décision entraîné précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from itertools import combinations\n",
    "\n",
    "attr = list(combinations(df.columns[:-2], 2))\n",
    "attr = list(map(list, attr)) # List of all pairs of attributes in the iris dataset\n",
    "\n",
    "for i, pair in enumerate(attr):\n",
    "    X = df[pair].to_numpy()\n",
    "    y = iris.target\n",
    "    clf = tree.DecisionTreeClassifier().fit(X,y)\n",
    "    \n",
    "    ax = plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "    # Decision Boundaries\n",
    "    DecisionBoundaryDisplay.from_estimator(clf, X, cmap = \"brg\", response_method=\"predict\", ax=ax,\n",
    "                                           xlabel=pair[0],\n",
    "                                           ylabel=pair[1])    \n",
    "    # Data Points\n",
    "    for t, color in zip(range(3), \"brg\"):\n",
    "        idx = np.where(t == y)\n",
    "        plt.scatter(X[idx, 0],\n",
    "                    X[idx, 1],\n",
    "                    c = color,\n",
    "                    cmap = \"brg\",\n",
    "                    edgecolors=\"black\",\n",
    "                    s = 20)\n",
    "        \n",
    "plt.suptitle(\"Ensemble de frontières de décisions 2 à 2 données par un DecisionTree\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "1. Build 2 different trees based on a sepal features (sepal lengths, sepal widths) vs petal features (petal lengths, petal widths) only: which features are the most discriminant?\n",
    "\n",
    "2. Compare performances with those obtained using all features.\n",
    "\n",
    "3. Try the same as above using the various splitting criterion available, Gini's index, classification error or cross-entropy. Comment on your results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Réponse 1</u> : On crée deux arbres de décision basés sur les attributs liés aux sépales et ceux liés aux pétales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "y = pd.factorize(train['species'])[0]\n",
    "y_true = pd.factorize(test['species'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_sepal = [\"sepal length (cm)\", \"sepal width (cm)\"]\n",
    "clf_sepal = tree.DecisionTreeClassifier().fit(train[features_sepal], y)\n",
    "dot_data_sepal = tree.export_graphviz(clf_sepal, out_file=None, \n",
    "                         feature_names=features_sepal,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph_sepal = graphviz.Source(dot_data_sepal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_sepal = clf_sepal.predict(test[features_sepal])\n",
    "print(accuracy_score(y_true, y_pred_sepal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
    "df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "y = pd.factorize(train['species'])[0]\n",
    "clf = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_petal = [\"petal length (cm)\", \"petal width (cm)\"]\n",
    "clf_petal = clf.fit(train[features_petal], y)\n",
    "dot_data_petal = tree.export_graphviz(clf_petal, out_file=None, \n",
    "                         feature_names=features_petal,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph_petal = graphviz.Source(dot_data_petal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = pd.factorize(test['species'])[0]\n",
    "y_pred_petal = clf_petal.predict(test[features_petal])\n",
    "print(accuracy_score(y_true, y_pred_petal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further ahead (not mandatory) \n",
    "Try the same approach adapted to another toy dataset from `scikit-learn` described at:\n",
    "http://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "Play with another dataset available at:\n",
    "http://archive.ics.uci.edu/ml/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Random forests\n",
    "Go to \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html \n",
    "\n",
    "for a documentation about the `RandomForestClassifier` provided by `scikit-learn`.\n",
    "\n",
    "Since target values must be integers, we first need to transform labels into numbers as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['species'] contains the actual species names. Before we can use it,\n",
    "# we need to convert each species name into a digit. So, in this case there\n",
    "# are three species, which have been coded as 0, 1, or 2.\n",
    "y = pd.factorize(train['species'])[0]\n",
    "\n",
    "# View target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest Classifier. By convention, clf means 'Classifier'\n",
    "rf = RandomForestClassifier(n_jobs=2, random_state=0)\n",
    "\n",
    "# Train the Classifier to take the training features and learn how they relate\n",
    "# to the training y (the species)\n",
    "rf.fit(train[features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make predictions** and create actual english names for the plants for each predicted plant class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rf.predict(test[features])\n",
    "preds_names = pd.Categorical.from_codes(preds, iris.target_names)\n",
    "preds_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix unsing pandas:\n",
    "pd.crosstab(test['species'], preds, rownames=['Actual Species'], colnames=['Predicted Species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using random forests byproducts\n",
    "\n",
    "One of the interesting use cases for random forest is feature selection. One of the byproducts of trying lots of decision tree variations is that you can examine which variables are working best/worst in each tree.\n",
    "\n",
    "When a certain tree uses one variable and another doesn't, you can compare the value lost or gained from the inclusion/exclusion of that variable. The good random forest implementations are going to do that for you, so all you need to do is know which method or variable to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View feature importance\n",
    "While we don't get regression coefficients like with ordinary least squares (OLS), we do get a score telling us how important each feature was in classifying. This is one of the most powerful parts of random forests, because we can clearly see that petal width was more important in classification than sepal width.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a list of the features and their importance scores\n",
    "list(zip(train[features], rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. Comment on the feature importances with respect to your previous observations on decision trees above.\n",
    "\n",
    "2. Extract and visualize 5 trees belonging to the random forest using the attribute `estimators_` of the trained random forest classifier. Compare them. *Note that you may code a loop on extracted trees.*\n",
    "\n",
    "3. Study the influence of parameters like `max_depth`, `min_samples_leaf` and `min_samples_split`. Try to optimize them and explain your approach and choices.\n",
    "\n",
    "4. How is estimated the prediction error of a random forest ?\n",
    "*Indication: have a look at parameter `oob_score`.*\n",
    "What are out-of-bag samples ?\n",
    "\n",
    "5. What should you do when classes are not balanced in the dataset ? (that is when there are much more examples of one class than another)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: a small example of regression using random forests\n",
    "Random forest is capable of learning without carefully crafted data transformations. Take the the $f(x) = \\sin(x)$ function for example.\n",
    "\n",
    "Create some fake data and add a little noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2.5, 2.5, 1000)\n",
    "y = np.sin(x) + np.random.normal(0, .1, 1000)\n",
    "\n",
    "plt.plot(x,y,'ko',markersize=1,label='data')\n",
    "plt.plot(np.arange(-2.5,2.5,0.1),np.sin(np.arange(-2.5,2.5,0.1)),'r-',label='ref')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try and build a basic linear model to predict y using x we end up with a straight line that sort of bisects the sin(x) function. Whereas if we use a random forest, it does a much better job of approximating the sin(x) curve and we get something that looks much more like the true function.\n",
    "\n",
    "Based on this example, we will illustrate how the random forest isn't bound by linear constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "1. Apply random forests on this dataset for regression and compare performances with ordinary least squares regression.\n",
    "*Note that ordinay least square regression is available thanks to:\n",
    "from sklearn.linear_model import LinearRegression*\n",
    "\n",
    "2. Comment on your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indications:\n",
    "You may use half of points for training and others to test predictions. Then you will have an idea of how far the random forest predictor fits the sinus curve.\n",
    "\n",
    "To this aim, you will need to use the model `RandomForestRegressor`. Be careful that when only 1 feature `x` is used as an input, you will need to reshape it by `x.reshape(-1,1)` when using methods `fit` and `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrf = RandomForestRegressor(n_estimators=30, max_depth=4)\n",
    "regrf.fit(x[0::2].reshape(-1, 1),y[0::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indication\n",
    "One clever way to compare models when using `scikit-learn`is to make a loop on models as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators)]\n",
    "\n",
    "for model in models:\n",
    "    ...\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "### Decision trees\n",
    "http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "### Random forests\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "### Plot decision surface : using `plt.contourf`\n",
    "http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html#sphx-glr-auto-examples-tree-plot-iris-py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning trees: not available in scikit-learn.\n",
    "Since post-pruning of tree is not implemented in scikit-learn, you may think of coding your own pruning function. For instance, taking into account the numer of samples per leaf as proposed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pruning function (useful ?)\n",
    "def prune(decisiontree, min_samples_leaf = 1):\n",
    "    if decisiontree.min_samples_leaf >= min_samples_leaf:\n",
    "        raise Exception('Tree already more pruned')\n",
    "    else:\n",
    "        decisiontree.min_samples_leaf = min_samples_leaf\n",
    "        tree = decisiontree.tree_\n",
    "        for i in range(tree.node_count):\n",
    "            n_samples = tree.n_node_samples[i]\n",
    "            if n_samples <= min_samples_leaf:\n",
    "                tree.children_left[i]=-1\n",
    "                tree.children_right[i]=-1\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e86c5fcaeb7504a0c486c54f5e7f20bce8324b88f64f392f8b6244d9f0e8929"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
