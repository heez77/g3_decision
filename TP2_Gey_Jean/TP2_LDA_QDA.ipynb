{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP2 : Classification using Linear & Quadratic Discriminant Analysis\n",
    "\n",
    "First think of configuring your notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "# import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading synthetic data\n",
    "Load the training and test data sets |synth_train.txt| and\n",
    "|synth_test.txt| already used for Knn. Targets belong to {1,2} and entries belong to R^2.\n",
    "We have 100 training data samples and 200 test samples.\n",
    "\n",
    "* the 1st column contains the label of the class the sample, \n",
    "* columns 2 & 3 contain the coordinates of each sample in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('synth_train.txt')\n",
    "\n",
    "test = np.loadtxt('synth_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall about the main steps of discriminant analysis:\n",
    "* estimation of weights `pi_1` and `pi_2` for each class,\n",
    "* estimation of empirical means `mu_1` and `mu_2` for each class, \n",
    "* estimation of empirical covariance matrices  `sigma_1` and `sigma_2`,\n",
    "* computation of the common averaged covariance `sigma` (average of intra-class covariances),\n",
    "* computation of log-probabilities of belonging to each class,\n",
    "* decision of classification,\n",
    "* display results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO : linear & quadratic discriminant analysis (LDA & QDA)\n",
    "1. Implement a classifier using LDA of the data set. \n",
    "2. Then implement QDA classification.\n",
    "3. In each case (LDA & QDA) show the decision boundary and\n",
    "compute the error rate respectively for the training set and the test set. \n",
    "4. Compare and comment on your results with LDA and QDA.\n",
    "5. You may also compare your results to K nearest neighbours.\n",
    "\n",
    "_Indication 1 : matrices `sigma` are of size 2x2.\n",
    "More generally, be careful of the sizes of vectors and matrices you\n",
    "manipulate._\n",
    "\n",
    "_Indication 2 : to display the regions of decision, you may use:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx1=100 # number of samples for display\n",
    "Nx2=100\n",
    "x1=np.linspace(-2.5,1.5,Nx1)  # sampling of the x1 axis \n",
    "x2=np.linspace(-0.5,3.5,Nx2)  # sampling of the x2 axis\n",
    "[X1,X2]=np.meshgrid(x1,x2)  \n",
    "x=np.hstack((X1.flatten('F'),X2.flatten('F'))) # list of the coordinates of points on the grid\n",
    "#N = size(x,axis=0)\n",
    "\n",
    "# Then compute the sampled prediction class_L for each couple (X1,X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Y\n",
    "import pandas as pd\n",
    "\n",
    "class LDA():\n",
    "    def __init__(self, train, test):\n",
    "        self.train_df = pd.DataFrame(train,columns = ['classe', 'x1', 'x2'])\n",
    "        self.test_df = pd.DataFrame(test,columns = ['classe', 'x1', 'x2'])\n",
    "        \n",
    "        \n",
    "    def get_pi_estimators(self):\n",
    "        return [pi for pi in self.train_df.classe.value_counts(normalize=True, ascending=True).values]\n",
    "    \n",
    "    def get_mu_estimators(self):\n",
    "        classes = self.train_df.classe.unique().tolist()\n",
    "        mu = np.zeros((len(classes), 2))\n",
    "        for i, c in enumerate(classes):\n",
    "            mu[i] = self.train_df[self.train_df.classe==c][['x1', 'x2']].sum(axis=0).to_numpy() / self.train_df[self.train_df.classe==c].shape[0]\n",
    "        return mu\n",
    "    \n",
    "    def get_sigma_estimators(self):\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = self.train_df.classe.unique().tolist()\n",
    "        sigma_moy = np.zeros((2,2))\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_moy += sigma\n",
    "        \n",
    "        return sigma_moy/self.train_df.shape[0]\n",
    "    \n",
    "    def get_log_probabilities(self, df):\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) + x.T @ np.linalg.inv(sigma) @ mu[j].reshape((2,1)) - 1/2 * mu[j].reshape((2,1)).T @ np.linalg.inv(sigma) @ mu[j].reshape((2,1))\n",
    "            prediction[i] = y\n",
    "        return prediction\n",
    "    def classification(self, train=True):\n",
    "        if train:\n",
    "            df = self.train_df\n",
    "        else:\n",
    "            df = self.test_df\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        return np.argmin(prediction, axis=1)\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA(LDA):\n",
    "    def __init__(self, train, test):\n",
    "        super().__init__(train, test)\n",
    "        \n",
    "    \n",
    "    def get_sigma_estimators(self):\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = self.train_df.classe.unique().tolist()\n",
    "        sigma_list =[]\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_list.append(sigma/train_df_c.shape[0])\n",
    "        \n",
    "        return sigma_list\n",
    "    \n",
    "    def get_log_probabilities(self, df):\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) - 1/2 * np.log(np.linalg.det(sigma[j])) -1/2 *  (x - mu[j].reshape((2,1))).T @ np.linalg.inv(sigma[j]) @ (x - mu[j].reshape((2,1)))\n",
    "            prediction[i] = y\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO : LDA & QDA using scikit-learn module\n",
    "\n",
    "The module `scikit-learn` is dedicated to machine learning algorithms. Many of them are available in a simple manner. For LDA and QDA, have a look at the tutorial available at http://scikit-learn.org/stable/modules/lda_qda.html \n",
    "\n",
    "**Warning** : you may have a critical view of the way LDA and QDA are illustrated in the proposed example...\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
