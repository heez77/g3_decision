{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP2 : Classification using Linear & Quadratic Discriminant Analysis\n",
    "\n",
    "First think of configuring your notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading synthetic data\n",
    "Load the training and test data sets |synth_train.txt| and\n",
    "|synth_test.txt| already used for Knn. Targets belong to {1,2} and entries belong to R^2.\n",
    "We have 100 training data samples and 200 test samples.\n",
    "\n",
    "* the 1st column contains the label of the class the sample, \n",
    "* columns 2 & 3 contain the coordinates of each sample in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.loadtxt('synth_train.txt')\n",
    "\n",
    "test = np.loadtxt('synth_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall about the main steps of discriminant analysis:\n",
    "* estimation of weights `pi_1` and `pi_2` for each class,\n",
    "* estimation of empirical means `mu_1` and `mu_2` for each class, \n",
    "* estimation of empirical covariance matrices  `sigma_1` and `sigma_2`,\n",
    "* computation of the common averaged covariance `sigma` (average of intra-class covariances),\n",
    "* computation of log-probabilities of belonging to each class,\n",
    "* decision of classification,\n",
    "* display results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO : linear & quadratic discriminant analysis (LDA & QDA)\n",
    "1. Implement a classifier using LDA of the data set. \n",
    "2. Then implement QDA classification.\n",
    "3. In each case (LDA & QDA) show the decision boundary and\n",
    "compute the error rate respectively for the training set and the test set. \n",
    "4. Compare and comment on your results with LDA and QDA.\n",
    "5. You may also compare your results to K nearest neighbours.\n",
    "\n",
    "_Indication 1 : matrices `sigma` are of size 2x2.\n",
    "More generally, be careful of the sizes of vectors and matrices you\n",
    "manipulate._\n",
    "\n",
    "_Indication 2 : to display the regions of decision, you may use:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('figures'):\n",
    "    os.mkdir('figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we implement 2 class, LDA and QDA to create two classifiers : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import Y\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "class LDA():\n",
    "    \"\"\"This class implement the linear discriminant analysis specifically for the loaded dataset synth. This class need to be loaded with a train and test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, train, test):\n",
    "        self.train_df = pd.DataFrame(train,columns = ['classe', 'x1', 'x2'])\n",
    "        self.test_df = pd.DataFrame(test,columns = ['classe', 'x1', 'x2'])\n",
    "        self.type = \"LDA\"\n",
    "        \n",
    "        \n",
    "    def get_pi_estimators(self)->list:\n",
    "        \"\"\"Returns the pi estimators for each class.\n",
    "\n",
    "        Returns:\n",
    "            list: list of floats\n",
    "        \"\"\"\n",
    "        return [pi for pi in self.train_df.classe.value_counts(normalize=True, ascending=True).values]\n",
    "    \n",
    "    def get_mu_estimators(self)->np.ndarray:\n",
    "        \"\"\"Returns the mu estimators for each class.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: an array where each line returns the vector mu (estimator) for the concerned class\n",
    "        \"\"\"\n",
    "        classes = [1,2]\n",
    "        mu = np.zeros((len(classes), 2))\n",
    "        for i, c in enumerate(classes):\n",
    "            mu[i] = self.train_df[self.train_df.classe==c][['x1', 'x2']].sum(axis=0).to_numpy() / self.train_df[self.train_df.classe==c].shape[0]\n",
    "        return mu\n",
    "    \n",
    "    def get_sigma_estimators(self)->np.ndarray:\n",
    "        \"\"\"Returns the average sigma estimator.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Sigma array in dimension 2x2\n",
    "        \"\"\"\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = [1,2]\n",
    "        sigma_moy = np.zeros((2,2))\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_moy += sigma\n",
    "        \n",
    "        return sigma_moy/self.train_df.shape[0]\n",
    "    \n",
    "    def get_log_probabilities(self, df:pd.DataFrame)->np.ndarray:\n",
    "        \"\"\"Compute the log_probability for the entries.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with the columns x1 and x2\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matrix in dim Nx2 where N is the shape of entries\n",
    "        \"\"\"\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) + x.T @ la.inv(sigma) @ mu[j].reshape((2,1)) - 1/2 * mu[j].reshape((2,1)).T @ la.inv(sigma) @ mu[j].reshape((2,1))\n",
    "            prediction[i] = y\n",
    "        return prediction\n",
    "    \n",
    "    def classification(self, train=True)->np.ndarray:\n",
    "        \"\"\"Returns the classification using discriminant analysis.\n",
    "\n",
    "        Args:\n",
    "            train (bool, optional): Use the trainset if True and the testset if not. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Vector with class id for each entry.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            df = self.train_df\n",
    "        else:\n",
    "            df = self.test_df\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        return np.argmax(prediction, axis=1) + 1\n",
    "    \n",
    "    def error_rate(self, train=True)->float:\n",
    "        classes = self.classification(train=train)\n",
    "        if train:\n",
    "            results = (classes == self.train_df.classe.to_numpy())\n",
    "            error_rate = 1 - np.count_nonzero(results)/self.train_df.shape[0]\n",
    "        else:\n",
    "            results = (classes == self.test_df.classe.to_numpy())\n",
    "            error_rate = 1 - np.count_nonzero(results)/self.test_df.shape[0]\n",
    "        return error_rate\n",
    "        \n",
    "    \n",
    "    def plot_decision_boundary(self):\n",
    "        \"\"\"Plot the decision boundary\n",
    "        \"\"\"\n",
    "        Nx1=100 # number of samples for display\n",
    "        Nx2=100\n",
    "        x1=np.linspace(-2.5,1.5,Nx1)  # sampling of the x1 axis \n",
    "        x2=np.linspace(-0.5,3.5,Nx2)  # sampling of the x2 axis\n",
    "        [X1,X2]=np.meshgrid(x1,x2)  \n",
    "        df = pd.DataFrame({'x1': X1.flatten('F'), 'x2': X2.flatten('F')})\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        classe = list(np.argmax(prediction, axis=1) + 1)\n",
    "        df['classe'] = [f'classe {i}' for i in classe]\n",
    "        fig = px.scatter(df, x=\"x1\", y=\"x2\", color = \"classe\", title=f\"Decision boundary with {self.type}\")\n",
    "        fig.write_image(f\"figures/decision_boundary_with_{self.type}.png\")\n",
    "        fig.show()\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDA(LDA):\n",
    "    \"\"\"This class implement the linear discriminant analysis specifically for the loaded dataset synth. This class need to be loaded with a train and test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, train, test):\n",
    "        super().__init__(train, test)\n",
    "        self.type = \"QDA\"\n",
    "    \n",
    "    def get_sigma_estimators(self)->list:\n",
    "        \"\"\"Returns the sigma estimator for each class.\n",
    "\n",
    "        Returns:\n",
    "            list: List of matrix in dim 2x2\n",
    "        \"\"\"\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = [1,2]\n",
    "        sigma_list =[]\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_list.append(sigma/train_df_c.shape[0])\n",
    "        \n",
    "        return sigma_list\n",
    "    \n",
    "    def get_log_probabilities(self, df):\n",
    "        \"\"\"Compute the log_probability for the entries.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with the columns x1 and x2\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matrix in dim Nx2 where N is the shape of entries\n",
    "        \"\"\"\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) - 1/2 * np.log(la.det(sigma[j])) -1/2 *  (x - mu[j].reshape((2,1))).T @ la.inv(sigma[j]) @ (x - mu[j].reshape((2,1)))\n",
    "            prediction[i] = y\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize our classifiers :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_classifier = LDA(train, test)\n",
    "qda_classifier = QDA(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can plot the decision boundary : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_classifier.plot_decision_boundary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_classifier.plot_decision_boundary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a huge difference between LDA and QDA. In LDA, the separation between class 1 and 2 is a linear function  while with the QDA the separation is no longer a linear function (because of the quadratic term in x when we compute the log probability). This is exactly what we have pointed out in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_LDA_train = lda_classifier.error_rate()\n",
    "error_rate_QDA_train = qda_classifier.error_rate()\n",
    "error_rate_LDA_test = lda_classifier.error_rate(train=False)\n",
    "error_rate_QDA_test = qda_classifier.error_rate(train=False)\n",
    "\n",
    "df_error_rate = pd.DataFrame(dict(train=[error_rate_LDA_train, error_rate_QDA_train], test=[error_rate_LDA_test, error_rate_QDA_test]), index=['LDA', 'QDA'])\n",
    "df_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training dataset, we get better results with the LDA but for the test dataset it is the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qda_classifier.get_sigma_estimators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the two covariance matrices, we notice how different they are, so LDA suffers from high bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(train[:,1:], train[:,0])\n",
    "error_rate_K_train = 1 - np.count_nonzero(neigh.predict(train[:,1:]) == train[:,0])/train.shape[0]\n",
    "error_rate_K_test = 1 - np.count_nonzero(neigh.predict(test[:,1:]) == test[:,0])/test.shape[0]\n",
    "df_error_rate.loc['K_neighbors'] = [error_rate_K_train, error_rate_K_test]\n",
    "df_error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nx1=100 # number of samples for display\n",
    "Nx2=100\n",
    "x1=np.linspace(-2.5,1.5,Nx1)  # sampling of the x1 axis \n",
    "x2=np.linspace(-0.5,3.5,Nx2)  # sampling of the x2 axis\n",
    "[X1,X2]=np.meshgrid(x1,x2)  \n",
    "df_k_neighbors_boundary = pd.DataFrame({'x1': X1.flatten('F'), 'x2': X2.flatten('F')})\n",
    "classe = list(neigh.predict(np.vstack((X1.flatten('F'), X2.flatten('F'))).T))\n",
    "df_k_neighbors_boundary['classe'] = [f'classe {i}' for i in classe]\n",
    "fig = px.scatter(df_k_neighbors_boundary, x=\"x1\", y=\"x2\", color = \"classe\", title=\"Decision boundary with K_neighbors\")\n",
    "fig.write_image(f\"figures/decision_boundary_with_K_neighbors.png\")\n",
    "fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO : LDA & QDA using scikit-learn module\n",
    "\n",
    "The module `scikit-learn` is dedicated to machine learning algorithms. Many of them are available in a simple manner. For LDA and QDA, have a look at the tutorial available at http://scikit-learn.org/stable/modules/lda_qda.html \n",
    "\n",
    "**Warning** : you may have a critical view of the way LDA and QDA are illustrated in the proposed example...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "skl_LDA_classifier = LinearDiscriminantAnalysis(store_covariance=True)\n",
    "skl_QDA_classifier = QuadraticDiscriminantAnalysis(store_covariance=True)\n",
    "\n",
    "skl_LDA_classifier.fit(train[:,1:], train[:,0])\n",
    "skl_QDA_classifier.fit(train[:,1:], train[:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skl_LDA_boundary = pd.DataFrame({'x1': X1.flatten('F'), 'x2': X2.flatten('F')})\n",
    "classe = list(skl_LDA_classifier.predict(np.vstack((X1.flatten('F'), X2.flatten('F'))).T))\n",
    "df_skl_LDA_boundary['classe'] = [f'classe {i}' for i in classe]\n",
    "fig = px.scatter(df_skl_LDA_boundary, x=\"x1\", y=\"x2\", color = \"classe\", title=\"Decision boundary with LDA from sklearn\")\n",
    "fig.write_image(f\"figures/decision_boundary_LDA_sklearn.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skl_QDA_boundary = pd.DataFrame({'x1': X1.flatten('F'), 'x2': X2.flatten('F')})\n",
    "classe = list(skl_QDA_classifier.predict(np.vstack((X1.flatten('F'), X2.flatten('F'))).T))\n",
    "df_skl_QDA_boundary['classe'] = [f'classe {i}' for i in classe]\n",
    "fig = px.scatter(df_skl_QDA_boundary, x=\"x1\", y=\"x2\", color = \"classe\", title=\"Decision boundary with with QDA from sklearn\")\n",
    "fig.write_image(f\"figures/decision_boundary_QDA_sklearn.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_LDA_skl_train = 1 - np.count_nonzero(skl_LDA_classifier.predict(train[:,1:]) == train[:,0])/train.shape[0]\n",
    "error_rate_LDA_skl_test = 1 - np.count_nonzero(skl_LDA_classifier.predict(test[:,1:]) == test[:,0])/test.shape[0]\n",
    "df_error_rate.loc['LDA with sklearn'] = [error_rate_LDA_skl_train, error_rate_LDA_skl_test]\n",
    "\n",
    "error_rate_QDA_skl_train = 1 - np.count_nonzero(skl_QDA_classifier.predict(train[:,1:]) == train[:,0])/train.shape[0]\n",
    "error_rate_QDA_skl_test = 1 - np.count_nonzero(skl_QDA_classifier.predict(test[:,1:]) == test[:,0])/test.shape[0]\n",
    "df_error_rate.loc['QDA with sklearn'] = [error_rate_QDA_skl_train, error_rate_QDA_skl_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same results as for our implementation, which is logical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rate.to_csv('error_rate_comparison.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
