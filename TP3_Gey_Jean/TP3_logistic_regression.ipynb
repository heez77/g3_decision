{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP3 : Logistic regression\n",
    "\n",
    "The purpose of this tutorial is to implement and use the Logistic Regression for binary classification. We will apply this\n",
    "method to the problem of handwritten characters to learn how to\n",
    "distinguish two numbers (here 5 and 6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression, IRLS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary question: the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the function `regression_logistique.m` and locate the main steps of the algorithm you have been taught (see course).\n",
    "You can comment the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_logistique(X,t,Nitermax=20,eps_conv=1e-3):\n",
    "    '''Entrees :\n",
    "    X = [ones(N_train,1) x_train];\n",
    "    t = class_train \n",
    "    Nitermax = nombre maximale d'itérations (20 par défaut)\n",
    "    eps_conv = critère de convergence sur norm(w-w_old)/norm(w) ; \n",
    "    eps_conv=1e-3 par défaut\n",
    "    \n",
    "    Sorties : \n",
    "    w : vecteur des coefficients de régression logistique\n",
    "   Niter : nombre d'itérations utilisées effectivement\n",
    "   \n",
    "   Fonction de régression logistique pour la classification binaire.\n",
    "   \n",
    "   Utilisation :\n",
    "       Nitermax = 50\n",
    "       eps_conv = 1e-4\n",
    "       [w,Niter] = regression_logistique(X,t,Nitermax,eps_conv)\n",
    "    '''\n",
    "    N_train = X.shape[0]\n",
    "\n",
    "    #initialisation : 1 pas de l'algorithme IRLS\n",
    "    w = np.zeros((X.shape[1],))\n",
    "    w_old = w \n",
    "    y = 1/2*np.ones((N_train,))\n",
    "    R = np.diag(y*(1-y))   # diag(y_n(1-y_n))\n",
    "    z = X.dot(w_old)-la.inv(R).dot(y-t)\n",
    "    w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "\n",
    "    # boucle appliquant l'algortihme de Newton-Raphson\n",
    "    Niter = 1\n",
    "    while ( (la.norm(w-w_old)/la.norm(w)>eps_conv) | (Niter<Nitermax) ):\n",
    "        Niter = Niter+1\n",
    "        y = 1/(1+np.exp(-X.dot(w)))\n",
    "        R = np.diag(y*(1-y))  \n",
    "        w_old = w \n",
    "        z = X.dot(w_old)-la.inv(R).dot(y-t) \n",
    "        w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "         \n",
    "    return w, Niter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading & preparing synthetic data\n",
    "\n",
    "Load the training and test data sets `synth_train.txt`\n",
    "and `synth_test.txt`. The targets t belong to {1,2} and the features  \n",
    "x belong to R^2. \n",
    "\n",
    "We have 100 training samples and 200 test samples\n",
    "\n",
    "* the 1st column contains the label of each sample, \n",
    "* columns 2 and 3 contain the coordinate of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "synth_train = np.loadtxt('synth_train.txt') \n",
    "class_train = synth_train[:,0]\n",
    "class_train_1 = np.where(synth_train[:,0]==1)[0]\n",
    "class_train_2 = np.where(synth_train[:,0]==2)[0]\n",
    "x_train = synth_train[:,1:]\n",
    "N_train = np.size(x_train,axis=0)\n",
    "\n",
    "# Test set\n",
    "synth_test = np.loadtxt('synth_test.txt')\n",
    "class_test = synth_test[:,0]\n",
    "class_test_1 = np.where(synth_test[:,0]==1)[0]\n",
    "class_test_2 = np.where(synth_test[:,0]==2)[0]\n",
    "x_test = synth_test[:,1:]\n",
    "N_test = np.size(x_test,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "px.scatter(pd.DataFrame(synth_train, columns = ['classe', 'x1', 'x2']),x='x1', y='x2', color='classe', title = 'Data visualization of training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(synth_test, columns = ['classe', 'x1', 'x2']),x='x1', y='x2', color='classe', title = 'Data visualization of test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing features for logistic regression (binary classification)\n",
    "First, we prepare the feature matrix and the target vector associated to \n",
    "the training and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((N_train,1)),x_train))\n",
    "t = 2-class_train   # 0 if class=2, 1 if class=1\n",
    "\n",
    "X_test = np.hstack((np.ones((N_test,1)),x_test))\n",
    "t_test = 2-class_test   # 0 if class=2, 1 if class=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 : the logistic function of decision\n",
    "\n",
    "1. Use the function `regression_logistique.m` to estimate the logistic\n",
    "regression vector `w`. *Indication : use `Nitermax = 50;\n",
    "eps_conv=1e-3;`.*\n",
    "2. Compute the decision function $f(x) = argmax_k P(C_k|x)$ on the test set\n",
    "to get the classification results. Recall that $y_n=\\sigma(w^T x)$ (logistic function)\n",
    "and that *using vectors* you may directly write $y=\\sigma(Xw)$, with the\n",
    "column of ones in X.\n",
    "3. Display the results by plotting the points from both the training set\n",
    "and the test set.\n",
    "4. Write the equation which defines the decision boundary.\n",
    "5. Artificially add a few points to the training set far from the decision boundary to check the robustness of logistic regression to outliers. Check the behaviour of LDA for comparison in this case and comment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x266f7a6aa70>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Exercise 1\n",
    "w, Niter = regression_logistique(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lambda x: np.where(np.exp(x)/( 1 + np.exp(x))<=0.5, 0 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_pred = predictions(X @ w)\n",
    "t_test_pred = predictions(X_test @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(prediction=2-t_train_pred, x1 = synth_train[:,1], x2 = synth_train[:,2])),x='x1', y='x2', color='prediction', title = 'Data visualization of predictions for training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(prediction=2-t_test_pred, x1 = synth_test[:,1], x2 = synth_test[:,2])),x='x1', y='x2', color='prediction', title = 'Data visualization of predictions for training dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can draw the decision boundary $w^Tx = 0$ by using: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# First compute w... then: \n",
    "x1 = np.linspace(-2.5,1.5,10) \n",
    "x2 = (-w[0]-w[1]*x1)/w[2]\n",
    "fig1 = px.line(pd.DataFrame(dict(x1=x1, x2=x2)), x='x1', y='x2')\n",
    "fig2 = px.scatter(pd.DataFrame(dict(prediction=2-t_train_pred, x1 = synth_train[:,1], x2 = synth_train[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary = go.Figure(data = fig1.data + fig2.data)\n",
    "fig_boundary.update_layout(dict(title='Decision Boundary with predictions of training dataset'))\n",
    "fig_boundary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_test = px.scatter(pd.DataFrame(dict(prediction=2-t_test_pred, x1 = synth_test[:,1], x2 = synth_test[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary_test = go.Figure(data = fig1.data + fig2_test.data)\n",
    "fig_boundary_test.update_layout(dict(title='Decision Boundary with predictions of test dataset'))\n",
    "fig_boundary_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding artificial outliers : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "n = 16\n",
    "norm.rvs(loc = 0, scale = 0.05, size = n).reshape((n//2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_points = 20\n",
    "outliers_points = np.repeat([[-30,40],[2,-4]],[nb_points,nb_points], axis=0) + norm.rvs(loc = 0, scale = 0.05, size = nb_points*4).reshape((nb_points*2,2))\n",
    "classe_outliers = np.concatenate((np.full(nb_points,1), np.full(nb_points,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(x1=outliers_points[:,0], x2=outliers_points[:,1], classe=classe_outliers)),\n",
    "           x='x1',\n",
    "           y='x2',\n",
    "           color='classe',\n",
    "           title = 'Outliers who where be add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = np.hstack((np.ones((outliers_points.shape[0],1)),outliers_points))\n",
    "t_outliers = 2-classe_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_outliers, Niter_outliers = regression_logistique(np.concatenate((X, X_outliers)), np.concatenate((t, t_outliers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_outliers_pred = predictions(np.concatenate((X, X_outliers)) @ w_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_outliers = (-w_outliers[0]-w_outliers[1]*x1)/w_outliers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1_outliers = px.line(pd.DataFrame(dict(x1=x1, x2=x2)), x='x1', y='x2')\n",
    "fig2_outliers = px.scatter(pd.DataFrame(dict(prediction=2-t_train_outliers_pred, x1 = np.concatenate((X, X_outliers))[:,1], x2 = np.concatenate((X, X_outliers))[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary_outliers = go.Figure(data = fig1_outliers.data + fig2_outliers.data)\n",
    "fig_boundary_outliers.update_layout(dict(title='Decision Boundary with predictions of training dataset'))\n",
    "fig_boundary_outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application: handwritten digits recognition 5 & 6\n",
    "We load 2 matrices which contain each a sequence of examples of 16x16 images \n",
    "of handwritten digits which are 5 and 6 here. Each line of the matrix\n",
    "contains 256 pixel values coding for the gray level of a 16x16 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_5 = np.loadtxt('train_5.txt',delimiter=',')   # 556 samples\n",
    "train_6 = np.loadtxt('train_6.txt',delimiter=',')   # 664 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 5\n",
    "n=9;\n",
    "I = np.reshape(train_5[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 6\n",
    "n=5;\n",
    "I = reshape(train_6[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the training and test sets\n",
    "\n",
    "We keep in the training set the 145 first images of 5s and the 200 first\n",
    "images of 6s:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_brut = np.vstack((train_5[:145,:], train_6[:200,:]))\n",
    "N_train = np.size(x_train_brut,axis=0)\n",
    "class_train = np.ones((345,1))   # label 1 for digit 6\n",
    "class_train[:145] = 0       # label 0 for digit 5\n",
    "\n",
    "x_test_brut = np.vstack((train_5[145:,:], train_6[200:,:]))\n",
    "N_test = np.size(train_5,axis=0)+np.size(train_6,axis=0)-N_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: logistic regression to classify 5 & 6\n",
    "\n",
    "1. Note that pixel values are between -1 and 1 by using the functions\n",
    " `min(I(:))` and `max(I(:))`.\n",
    "2. Identify the indices of the most significant pixels, which are defined \n",
    "as having a standard deviation greater than 0.5 here. We denote by `lis_sig`\n",
    "the list of positions of these significant pixels in the image vector.\n",
    "_Indication : the function `std` gives the standard deviation (columnwise\n",
    "in matrices) and you should find 173 pixel positions.\n",
    "3. Show a binary image to locate these pixels.\n",
    "_Indication : `Isig = zeros(16); Isig(list_sig)=1; Isig=Isig';`._\n",
    "4. Define the training set `x_train` from `x_train_brut` from the significant pixels only.\n",
    "5. Do the same with `x_test_brut` to extract `x_test`.\n",
    "6. Use `regression_logistique.m` to estimate the logistic regression vector\n",
    "`w` from the training set `x_train`. \n",
    "Choose `Nitermax = 13; eps_conv = 1e-3;`\n",
    "7. Compute the decision function and the labels of the test set `x_test`. \n",
    "_Indication : do not forget the column of ones !_\n",
    "8. Estimate the classification error rate by using :\n",
    "`erreur = sum(abs(class-class_test))/N_test;`.\n",
    "9. Locate some misclassified examples and visualize the corresponding image.\n",
    "Comment on your results and observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x266fb4b1b70>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_sig = np.where(np.std(x_train_brut, axis=0) > 0.5)[0].tolist()\n",
    "\n",
    "Isig = np.zeros(256)\n",
    "Isig[list_sig]=1\n",
    "Isig = np.reshape(np.array(Isig), (16,16))\n",
    "plt.imshow(Isig)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Logistic regression using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. **Go to** http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html for a presentation of the logistic regression model in `scikit-learn`.\n",
    "\n",
    "2. **Apply** it to the present data set.\n",
    "\n",
    "3. **Comment** on the use of logistic regression.\n",
    "\n",
    "*Indication : you may have a look at* \n",
    "\n",
    "a) Theory : http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html\n",
    "\n",
    "b) Video :  https://www.coursera.org/learn/machine-learning/lecture/4BHEy/regularized-logistic-regression \n",
    "\n",
    "c) Example : http://scikit-learn.org/stable/auto_examples/exercises/plot_digits_classification_exercise.html#sphx-glr-auto-examples-exercises-plot-digits-classification-exercise-py\n",
    "\n",
    "*for a short presentation of regularized logistic regression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [100, 345]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\geyma\\OneDrive\\Documents\\GitHub\\g3_decision\\TP3_Gey_Jean\\TP3_logistic_regression.ipynb Cellule 26\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/geyma/OneDrive/Documents/GitHub/g3_decision/TP3_Gey_Jean/TP3_logistic_regression.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/geyma/OneDrive/Documents/GitHub/g3_decision/TP3_Gey_Jean/TP3_logistic_regression.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/geyma/OneDrive/Documents/GitHub/g3_decision/TP3_Gey_Jean/TP3_logistic_regression.ipynb#X34sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf \u001b[39m=\u001b[39m LogisticRegression(random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mfit(x_train, class_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/geyma/OneDrive/Documents/GitHub/g3_decision/TP3_Gey_Jean/TP3_logistic_regression.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m w \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mcoef_[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/geyma/OneDrive/Documents/GitHub/g3_decision/TP3_Gey_Jean/TP3_logistic_regression.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m bias \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mintercept_[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\geyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1508\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1506\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1508\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1509\u001b[0m     X,\n\u001b[0;32m   1510\u001b[0m     y,\n\u001b[0;32m   1511\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1512\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1513\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1514\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1515\u001b[0m )\n\u001b[0;32m   1516\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1517\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\geyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\geyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:981\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    964\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     estimator\u001b[39m=\u001b[39mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[0;32m    979\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric)\n\u001b[1;32m--> 981\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m    983\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\geyma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 345]"
     ]
    }
   ],
   "source": [
    "# Include your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=True).fit(x_train, class_train)\n",
    "w = clf.coef_[0]\n",
    "bias = clf.intercept_[0]\n",
    "print(f\"Prédiction : {clf.predict(x_test)}\")\n",
    "print(f\"Score : {clf.score(x_test, class_test)*100}%\")\n",
    "print(f\"Coefficients de la régression : {bias, w}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à la méthode de régression par scikit-learn, on trouve une fonction de régression de la forme :\n",
    "\n",
    "$y = 2.63*x[0] - 2.12*x[1] + 5.46$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
