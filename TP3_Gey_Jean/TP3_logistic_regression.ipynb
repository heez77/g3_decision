{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP3 : Logistic regression\n",
    "\n",
    "The purpose of this tutorial is to implement and use the Logistic Regression for binary classification. We will apply this\n",
    "method to the problem of handwritten characters to learn how to\n",
    "distinguish two numbers (here 5 and 6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression, IRLS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary question: the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the function `regression_logistique.m` and locate the main steps of the algorithm you have been taught (see course).\n",
    "You can comment the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_logistique(X,t,Nitermax=20,eps_conv=1e-3):\n",
    "    '''Entrees :\n",
    "    X = [ones(N_train,1) x_train];\n",
    "    t = class_train \n",
    "    Nitermax = nombre maximale d'itérations (20 par défaut)\n",
    "    eps_conv = critère de convergence sur norm(w-w_old)/norm(w) ; \n",
    "    eps_conv=1e-3 par défaut\n",
    "    \n",
    "    Sorties : \n",
    "    w : vecteur des coefficients de régression logistique\n",
    "   Niter : nombre d'itérations utilisées effectivement\n",
    "   \n",
    "   Fonction de régression logistique pour la classification binaire.\n",
    "   \n",
    "   Utilisation :\n",
    "       Nitermax = 50\n",
    "       eps_conv = 1e-4\n",
    "       [w,Niter] = regression_logistique(X,t,Nitermax,eps_conv)\n",
    "    '''\n",
    "    N_train = X.shape[0]\n",
    "\n",
    "    #initialisation : 1 pas de l'algorithme IRLS\n",
    "    w = np.zeros((X.shape[1],))\n",
    "    w_old = w \n",
    "    y = 1/2*np.ones((N_train,))\n",
    "    R = np.diag(y*(1-y))   # diag(y_n(1-y_n))\n",
    "    \n",
    "    z = X.dot(w_old)-la.inv(R).dot(y-t)\n",
    "    w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "\n",
    "    # boucle appliquant l'algortihme de Newton-Raphson\n",
    "    Niter = 1\n",
    "    while ( (la.norm(w-w_old)/la.norm(w)>eps_conv) | (Niter<Nitermax) ):\n",
    "        Niter = Niter+1\n",
    "        y = 1/(1+np.exp(-X.dot(w)))\n",
    "        R = np.diag(y*(1-y))\n",
    "        w_old = w \n",
    "        z = X.dot(w_old)-la.inv(R).dot(y-t) \n",
    "        w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "         \n",
    "    return w, Niter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an error in the code given with `|` which must be replaced by `and`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code, we assume that the matrix R is invertible, which, as we will see in the rest of the tutorial, is not necessarily the case. This is a first limitation of the implementation proposed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading & preparing synthetic data\n",
    "\n",
    "Load the training and test data sets `synth_train.txt`\n",
    "and `synth_test.txt`. The targets t belong to {1,2} and the features  \n",
    "x belong to R^2. \n",
    "\n",
    "We have 100 training samples and 200 test samples\n",
    "\n",
    "* the 1st column contains the label of each sample, \n",
    "* columns 2 and 3 contain the coordinate of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "synth_train = np.loadtxt('synth_train.txt') \n",
    "class_train = synth_train[:,0]\n",
    "class_train_1 = np.where(synth_train[:,0]==1)[0]\n",
    "class_train_2 = np.where(synth_train[:,0]==2)[0]\n",
    "x_train = synth_train[:,1:]\n",
    "N_train = np.size(x_train,axis=0)\n",
    "\n",
    "# Test set\n",
    "synth_test = np.loadtxt('synth_test.txt')\n",
    "class_test = synth_test[:,0]\n",
    "class_test_1 = np.where(synth_test[:,0]==1)[0]\n",
    "class_test_2 = np.where(synth_test[:,0]==2)[0]\n",
    "x_test = synth_test[:,1:]\n",
    "N_test = np.size(x_test,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "px.scatter(pd.DataFrame(synth_train, columns = ['classe', 'x1', 'x2']),x='x1', y='x2', color='classe', title = 'Data visualization of training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(synth_test, columns = ['classe', 'x1', 'x2']),x='x1', y='x2', color='classe', title = 'Data visualization of test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same dataset as in the previous tutorial, the test dataset is still less well separated than the training dataset, which will necessarily result in a decrease in performance on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing features for logistic regression (binary classification)\n",
    "First, we prepare the feature matrix and the target vector associated to \n",
    "the training and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((N_train,1)),x_train))\n",
    "t = 2-class_train   # 0 if class=2, 1 if class=1\n",
    "\n",
    "X_test = np.hstack((np.ones((N_test,1)),x_test))\n",
    "t_test = 2-class_test   # 0 if class=2, 1 if class=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 : the logistic function of decision\n",
    "\n",
    "1. Use the function `regression_logistique.m` to estimate the logistic\n",
    "regression vector `w`. *Indication : use `Nitermax = 50;\n",
    "eps_conv=1e-3;`.*\n",
    "2. Compute the decision function $f(x) = argmax_k P(C_k|x)$ on the test set\n",
    "to get the classification results. Recall that $y_n=\\sigma(w^T x)$ (logistic function)\n",
    "and that *using vectors* you may directly write $y=\\sigma(Xw)$, with the\n",
    "column of ones in X.\n",
    "3. Display the results by plotting the points from both the training set\n",
    "and the test set.\n",
    "4. Write the equation which defines the decision boundary.\n",
    "5. Artificially add a few points to the training set far from the decision boundary to check the robustness of logistic regression to outliers. Check the behaviour of LDA for comparison in this case and comment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1\n",
    "w, Niter = regression_logistique(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lambda x: np.where(np.exp(x)/( 1 + np.exp(x))<=0.5, 0 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_pred = predictions(X @ w)\n",
    "t_test_pred = predictions(X_test @ w)\n",
    "classes_pred_train = [f\"Good prediction for classe {t_pred+1}\" if t_pred==t[i] else f\"Predicted classe {t_pred+1} but actually classe {int(t[i])+1}\" for i, t_pred in enumerate(t_train_pred)]\n",
    "classes_pred_test = [f\"Good prediction for classe {t_pred+1}\" if t_pred==t_test[i] else f\"Predicted classe {t_pred+1} but actually classe {int(t_test[i])+1}\" for i, t_pred in enumerate(t_test_pred)]\n",
    "\n",
    "color_map = {\"Good prediction for classe 1 \": \"#636EFA\",\n",
    "             \"Good predictions for classe 2\": \"#FECB52\",\n",
    "             \"Predicted classe 1 but actually classe 2\": \"#D62728\",\n",
    "             \"Predicted classe 2 but actually classe 1\": \"#2CA02C\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_prediction_train = px.scatter(pd.DataFrame(dict(prediction=classes_pred_train, x1 = synth_train[:,1], x2 = synth_train[:,2])),x='x1', y='x2', color='prediction',\n",
    "           title = 'Data visualization of predictions for training dataset',\n",
    "           color_discrete_map=color_map)\n",
    "\n",
    "fig_prediction_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_prediction_test = px.scatter(pd.DataFrame(dict(prediction=classes_pred_test, x1 = synth_test[:,1], x2 = synth_test[:,2])),x='x1', y='x2', color='prediction', \n",
    "           title = 'Data visualization of predictions for training dataset',\n",
    "            color_discrete_map=color_map)\n",
    "fig_prediction_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can draw the decision boundary $w^Tx = 0$ by using: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# First compute w... then: \n",
    "x1 = np.linspace(-2.5,1.5,10) \n",
    "x2 = (-w[0]-w[1]*x1)/w[2]\n",
    "fig1 = px.line(pd.DataFrame(dict(x1=x1, x2=x2)), x='x1', y='x2')\n",
    "fig_boundary = go.Figure(data = fig1.data + fig_prediction_train.data)\n",
    "fig_boundary.update_layout(dict(title='Decision Boundary with predictions of training dataset'))\n",
    "fig_boundary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_test = px.scatter(pd.DataFrame(dict(prediction=2-t_test_pred, x1 = synth_test[:,1], x2 = synth_test[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary_test = go.Figure(data = fig1.data + fig_prediction_test.data)\n",
    "fig_boundary_test.update_layout(dict(title='Decision Boundary with predictions of test dataset'))\n",
    "fig_boundary_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary is an affine function as for LDA. We can compare the results between logistic regression and LDA to see if one of them is better on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_train = 1 - np.count_nonzero(t_train_pred==t) / t.shape[0]\n",
    "error_rate_test = 1 - np.count_nonzero(t_test_pred==t_test) / t_test.shape[0]\n",
    "\n",
    "df_error_rate_comparison = pd.read_csv('error_rate_comparison.csv', index_col = 'Unnamed: 0', sep=';')\n",
    "\n",
    "df_error_rate_comparison.loc['Logistic Regression'] = [error_rate_train, error_rate_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rate_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the LDA obtained better results for the test dataset than the logistic regression. However, we had seen that LDA was extremely sensitive to outliers, is this the case for logistic regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding artificial outliers : \n",
    "\n",
    "With the current implementation of the algorithm, we cannot add outliers that are too far from the data, otherwise the R matrix does not become invertible and the algorithm crashes. For now, we will take outliers that do not make the R matrix non-invertible and then look at the difference between LDA and logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "nb_points = 10\n",
    "outliers_points = np.repeat([[2,-1]],[nb_points], axis=0) + norm.rvs(loc = 0, scale = 0.2, size = nb_points*2).reshape((nb_points,2))\n",
    "classe_outliers = np.full(nb_points,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(x1=outliers_points[:,0], x2=outliers_points[:,1], classe=classe_outliers)),\n",
    "           x='x1',\n",
    "           y='x2',\n",
    "           color='classe',\n",
    "           title = 'Outliers who where be add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = np.hstack((np.ones((outliers_points.shape[0],1)),outliers_points))\n",
    "t_outliers = 2-classe_outliers\n",
    "w_outliers, Niter_outliers = regression_logistique(np.concatenate((X, X_outliers)), np.concatenate((t, t_outliers)))\n",
    "t_train_outliers_pred = predictions(np.concatenate((X, X_outliers)) @ w_outliers)\n",
    "x2_outliers = (-w_outliers[0]-w_outliers[1]*x1)/w_outliers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1_outliers = px.line(pd.DataFrame(dict(x1=x1, x2=x2)), x='x1', y='x2')\n",
    "fig2_outliers = px.scatter(pd.DataFrame(dict(prediction=2-t_train_outliers_pred, x1 = np.concatenate((X, X_outliers))[:,1], x2 = np.concatenate((X, X_outliers))[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary_outliers = go.Figure(data = fig1.data + fig1_outliers.data + fig2_outliers.data)\n",
    "fig_boundary_outliers.update_layout(dict(title='Decision Boundary with predictions of training dataset'))\n",
    "fig_boundary_outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for the logistic regression, the decision frontier did not move at all. The use of the sigma function allows to limit the impact of the out:liers on the decision frontier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA implementation from TP2 : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA():\n",
    "    \"\"\"This class implement the linear discriminant analysis specifically for the loaded dataset synth. This class need to be loaded with a train and test dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, train, test):\n",
    "        self.train_df = pd.DataFrame(train,columns = ['classe', 'x1', 'x2'])\n",
    "        self.test_df = pd.DataFrame(test,columns = ['classe', 'x1', 'x2'])\n",
    "        self.type = \"LDA\"\n",
    "        \n",
    "        \n",
    "    def get_pi_estimators(self)->list:\n",
    "        \"\"\"Returns the pi estimators for each class.\n",
    "\n",
    "        Returns:\n",
    "            list: list of floats\n",
    "        \"\"\"\n",
    "        return [pi for pi in self.train_df.classe.value_counts(normalize=True, ascending=True).values]\n",
    "    \n",
    "    def get_mu_estimators(self)->np.ndarray:\n",
    "        \"\"\"Returns the mu estimators for each class.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: an array where each line returns the vector mu (estimator) for the concerned class\n",
    "        \"\"\"\n",
    "        classes = [1,2]\n",
    "        mu = np.zeros((len(classes), 2))\n",
    "        for i, c in enumerate(classes):\n",
    "            mu[i] = self.train_df[self.train_df.classe==c][['x1', 'x2']].sum(axis=0).to_numpy() / self.train_df[self.train_df.classe==c].shape[0]\n",
    "        return mu\n",
    "    \n",
    "    def get_sigma_estimators(self)->np.ndarray:\n",
    "        \"\"\"Returns the average sigma estimator.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Sigma array in dimension 2x2\n",
    "        \"\"\"\n",
    "        mu = self.get_mu_estimators()\n",
    "        classes = [1,2]\n",
    "        sigma_moy = np.zeros((2,2))\n",
    "        for i, c in enumerate(classes):\n",
    "            train_df_c = self.train_df[self.train_df.classe==c]\n",
    "            sigma = np.zeros((2,2))\n",
    "            for j in range(train_df_c.shape[0]):\n",
    "                xn = train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1))\n",
    "                sigma += ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ) @ ( train_df_c[['x1', 'x2']].iloc[j].to_numpy().reshape((2,1)) - mu[i].reshape((2,1)) ).T\n",
    "            sigma_moy += sigma\n",
    "        \n",
    "        return sigma_moy/self.train_df.shape[0]\n",
    "    \n",
    "    def get_log_probabilities(self, df:pd.DataFrame)->np.ndarray:\n",
    "        \"\"\"Compute the log_probability for the entries.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with the columns x1 and x2\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Matrix in dim Nx2 where N is the shape of entries\n",
    "        \"\"\"\n",
    "        pi = self.get_pi_estimators()\n",
    "        mu = self.get_mu_estimators()\n",
    "        sigma = self.get_sigma_estimators()\n",
    "        prediction = np.zeros((len(df), mu.shape[0]))\n",
    "        for i in range(df.shape[0]):\n",
    "            x = df[['x1', 'x2']].iloc[i].to_numpy().reshape((2,1))\n",
    "            y = np.zeros(mu.shape[0])\n",
    "            for j in range(mu.shape[0]):\n",
    "                y[j] = np.log(pi[j]) + x.T @ la.inv(sigma) @ mu[j].reshape((2,1)) - 1/2 * mu[j].reshape((2,1)).T @ la.inv(sigma) @ mu[j].reshape((2,1))\n",
    "            prediction[i] = y\n",
    "        return prediction\n",
    "    \n",
    "    def classification(self, train=True)->np.ndarray:\n",
    "        \"\"\"Returns the classification using discriminant analysis.\n",
    "\n",
    "        Args:\n",
    "            train (bool, optional): Use the trainset if True and the testset if not. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Vector with class id for each entry.\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            df = self.train_df\n",
    "        else:\n",
    "            df = self.test_df\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        return np.argmax(prediction, axis=1) + 1\n",
    "    \n",
    "    def error_rate(self, train=True)->float:\n",
    "        classes = self.classification(train=train)\n",
    "        if train:\n",
    "            results = (classes == self.train_df.classe.to_numpy())\n",
    "            error_rate = 1 - np.count_nonzero(results)/self.train_df.shape[0]\n",
    "        else:\n",
    "            results = (classes == self.test_df.classe.to_numpy())\n",
    "            error_rate = 1 - np.count_nonzero(results)/self.test_df.shape[0]\n",
    "        return error_rate\n",
    "        \n",
    "    \n",
    "    def plot_decision_boundary(self):\n",
    "        \"\"\"Plot the decision boundary\n",
    "        \"\"\"\n",
    "        Nx1=100 # number of samples for display\n",
    "        Nx2=100\n",
    "        x1=np.linspace(-2.5,1.5,Nx1)  # sampling of the x1 axis \n",
    "        x2=np.linspace(-0.5,3.5,Nx2)  # sampling of the x2 axis\n",
    "        [X1,X2]=np.meshgrid(x1,x2)  \n",
    "        df = pd.DataFrame({'x1': X1.flatten('F'), 'x2': X2.flatten('F')})\n",
    "        prediction = self.get_log_probabilities(df)\n",
    "        classe = list(np.argmax(prediction, axis=1) + 1)\n",
    "        df['classe'] = [f'classe {i}' for i in classe]\n",
    "        fig = px.scatter(df, x=\"x1\", y=\"x2\", color = \"classe\", title=f\"Decision boundary with {self.type}\")\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_classifier = LDA(np.concatenate((synth_train, np.concatenate((classe_outliers.reshape(-1,1), outliers_points), axis=1))), synth_test)\n",
    "fig_boundary_lda = lda_classifier.plot_decision_boundary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_boundary_outliers_lda = go.Figure(data = fig1.data + fig_boundary_lda.data)\n",
    "fig_boundary_outliers_lda.update_layout(dict(title='Decision Boundary with LDA and outliers'))\n",
    "fig_boundary_outliers_lda.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset used includes outliers, using LDA is really not the right choice (same for QDA) and in this case, it is better to use logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application: handwritten digits recognition 5 & 6\n",
    "We load 2 matrices which contain each a sequence of examples of 16x16 images \n",
    "of handwritten digits which are 5 and 6 here. Each line of the matrix\n",
    "contains 256 pixel values coding for the gray level of a 16x16 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_5 = np.loadtxt('train_5.txt',delimiter=',')   # 556 samples\n",
    "train_6 = np.loadtxt('train_6.txt',delimiter=',')   # 664 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 5\n",
    "n=9\n",
    "I = np.reshape(train_5[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 6\n",
    "n=5\n",
    "I = reshape(train_6[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images we have are in 16x16 size which returns an extremely pixilated image. Ideally, working with a better quality image should help to get better results. However, by using compressed images, we greatly reduce the computation time of the algorithm, so if the algorithm performs well with compressed images, we should not deprive ourselves! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the training and test sets\n",
    "\n",
    "We keep in the training set the 145 first images of 5s and the 200 first\n",
    "images of 6s:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_brut = np.vstack((train_5[:145,:], train_6[:200,:]))\n",
    "N_train = np.size(x_train_brut,axis=0)\n",
    "class_train = np.ones((345,1))   # label 1 for digit 6\n",
    "class_train[:145] = 0       # label 0 for digit 5\n",
    "class_train = np.squeeze(class_train)\n",
    "\n",
    "x_test_brut = np.vstack((train_5[145:,:], train_6[200:,:]))\n",
    "N_test = np.size(train_5,axis=0)+np.size(train_6,axis=0)-N_train\n",
    "class_test = np.ones((875,1))\n",
    "class_test[:(556-145)] = 0\n",
    "class_test = np.squeeze(class_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: logistic regression to classify 5 & 6\n",
    "\n",
    "1. Note that pixel values are between -1 and 1 by using the functions\n",
    " `min(I(:))` and `max(I(:))`.\n",
    "2. Identify the indices of the most significant pixels, which are defined \n",
    "as having a standard deviation greater than 0.5 here. We denote by `lis_sig`\n",
    "the list of positions of these significant pixels in the image vector.\n",
    "_Indication : the function `std` gives the standard deviation (columnwise\n",
    "in matrices) and you should find 173 pixel positions.\n",
    "3. Show a binary image to locate these pixels.\n",
    "_Indication : `Isig = zeros(16); Isig(list_sig)=1; Isig=Isig';`._\n",
    "4. Define the training set `x_train` from `x_train_brut` from the significant pixels only.\n",
    "5. Do the same with `x_test_brut` to extract `x_test`.\n",
    "6. Use `regression_logistique.m` to estimate the logistic regression vector\n",
    "`w` from the training set `x_train`. \n",
    "Choose `Nitermax = 13; eps_conv = 1e-3;`\n",
    "7. Compute the decision function and the labels of the test set `x_test`. \n",
    "_Indication : do not forget the column of ones !_\n",
    "8. Estimate the classification error rate by using :\n",
    "`erreur = sum(abs(class-class_test))/N_test;`.\n",
    "9. Locate some misclassified examples and visualize the corresponding image.\n",
    "Comment on your results and observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sig = np.where(np.std(x_train_brut, axis=0) > 0.5)[0].tolist()\n",
    "\n",
    "Isig = np.zeros(256)\n",
    "Isig[list_sig]=1\n",
    "Isig = np.reshape(np.array(Isig), (16,16))\n",
    "plt.imshow(Isig, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_brut[:, list_sig]\n",
    "x_test = x_test_brut[:, list_sig]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    w, Niter = regression_logistique(x_train, np.squeeze(class_train), Nitermax=13, eps_conv=1e-3)\n",
    "except:\n",
    "    print(\"Singular matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we have problems with the inversion of the R matrix, so we need to find a way around this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposition 1 : Use of a constant in the matrix R to make it invertible\n",
    "\n",
    "Proposition 2 : Stop the algorithm as soon as the matrix R is not invertible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class Logistique_Regression_early_stop():\n",
    "    \"\"\"This class implement the logistic regression using IRLS algorithm and an early stop if R matrix is not invertible.\n",
    "    \"\"\"\n",
    "    def __init__(self, train:np.ndarray, class_train:np.ndarray, test:np.ndarray, class_test:np.ndarray, Nitermax:int = 13, eps_conv:float=1e-3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (np.ndarray): training dataset \n",
    "            class_train (np.ndarray): respective class for the training dataset\n",
    "            test (np.ndarray): test dataset \n",
    "            class_test (np.ndarray): respective class for the test dataset\n",
    "            Nitermax (int, optional): The maximum number of iteration before the algorithm stop. Defaults to 13.\n",
    "            eps_conv (float, optional): The tolerance criterion before the algorithm stop. Defaults to 1e-3.\n",
    "        \"\"\"\n",
    "        self.train = train\n",
    "        self.class_train = class_train\n",
    "        self.N_train = train.shape[0]\n",
    "        self.test = test\n",
    "        self.class_test = class_test\n",
    "        self.N_test = test.shape[0]\n",
    "        self.Nitermax = Nitermax\n",
    "        self.eps_conv = eps_conv\n",
    "    \n",
    "    def preprocessing(self, value:float=0.5):\n",
    "        \"\"\"Do the preprocessing of the images before the training\n",
    "\n",
    "        Args:\n",
    "            value (float, optional): The threshold used to select pixels. Defaults to 0.5.\n",
    "        \"\"\"\n",
    "        lis_sig = np.where(np.std(self.train, axis=0) > value)[0].tolist()\n",
    "        self.x_train = self.train[:, lis_sig]\n",
    "        self.x_test = self.test[:, lis_sig]\n",
    "        self.X_train = np.hstack((np.ones((self.N_train,1)),self.x_train))\n",
    "        self.X_test = np.hstack((np.ones((self.N_test,1)),self.x_test))\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Implementation of the IRLS algorithm method. If the matrix R is no longer invertible, the algorithm stop. Returns w and the number of iterations.\n",
    "        \"\"\"\n",
    "        #initialisation : 1 pas de l'algorithme IRLS\n",
    "        t = self.class_train\n",
    "        X = self.X_train\n",
    "        w = np.zeros((self.X_train.shape[1],))\n",
    "        w_old = w \n",
    "        y = 1/2*np.ones((self.N_train,))\n",
    "        R = np.diag(y*(1-y))   # diag(y_n(1-y_n))\n",
    "        z = X.dot(w_old)-la.inv(R).dot(y-t)\n",
    "        w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "\n",
    "        # boucle appliquant l'algortihme de Newton-Raphson\n",
    "        Niter = 1\n",
    "        while ( (la.norm(w-w_old)/la.norm(w)>self.eps_conv) and (Niter<self.Nitermax) ):\n",
    "            Niter = Niter+1\n",
    "            y = 1/(1+np.exp(-X.dot(w)))\n",
    "            R = np.diag(y*(1-y))  \n",
    "            w_old = w \n",
    "            try:\n",
    "                z = X.dot(w_old)-la.inv(R).dot(y-t) \n",
    "                w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "            except:\n",
    "                break\n",
    "        self.w = w\n",
    "        self.Niter = Niter\n",
    "    def predict(self):\n",
    "        \"\"\"Make predictions on the test dataset\n",
    "        \"\"\"\n",
    "        predictions = lambda x: np.where(np.exp(x)/( 1 + np.exp(x))<=0.5, 0 , 1)\n",
    "        self.t_test_pred = predictions(self.X_test @self.w)\n",
    "\n",
    "    def confusion_matrix(self, show:bool=True)->np.ndarray:\n",
    "        \"\"\"Compute the confusion matrix\n",
    "\n",
    "        Args:\n",
    "            show (bool, optional): If true plot the confusion matrix. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Confusion matrix as an array\n",
    "        \"\"\"\n",
    "        cm = confusion_matrix(self.class_test, self.t_test_pred)\n",
    "        if show:\n",
    "            fig = px.imshow(cm, text_auto=True)\n",
    "            fig.show()\n",
    "        return cm\n",
    "\n",
    "class Logistique_Regression_with_constant(Logistique_Regression_early_stop):\n",
    "    def __init__(self, train:np.ndarray, class_train:np.ndarray, test:np.ndarray, class_test:np.ndarray, cst:float, Nitermax:int = 13, eps_conv:float=1e-3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (np.ndarray): training dataset.\n",
    "            class_train (np.ndarray): respective class for the training dataset.\n",
    "            test (np.ndarray): test dataset.\n",
    "            class_test (np.ndarray): respective class for the test dataset.\n",
    "            cst (float): The value to add to the R matrix if it is not invertible.\n",
    "            Nitermax (int, optional): The maximum number of iteration before the algorithm stop. Defaults to 13.\n",
    "            eps_conv (float, optional): The tolerance criterion before the algorithm stop. Defaults to 1e-3.\n",
    "        \"\"\"\n",
    "        super().__init__(train, class_train, test, class_test, Nitermax, eps_conv)\n",
    "        self.cst = cst\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"Implementation of the IRLS algorithm method. If the matrix R is no longer invertible, we had the constant value cst to the R matrix before before computing the inverse. Returns w and the number of iterations.\n",
    "        \"\"\"\n",
    "        #initialisation : 1 pas de l'algorithme IRLS\n",
    "        t = self.class_train\n",
    "        X = self.X_train\n",
    "        w = np.zeros((self.X_train.shape[1],))\n",
    "        w_old = w \n",
    "        y = 1/2*np.ones((self.N_train,))\n",
    "        R = np.diag(y*(1-y))   # diag(y_n(1-y_n))\n",
    "        z = X.dot(w_old)-la.inv(R).dot(y-t)\n",
    "        w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "\n",
    "        # boucle appliquant l'algortihme de Newton-Raphson\n",
    "        Niter = 1\n",
    "        while ( (la.norm(w-w_old)/la.norm(w)>self.eps_conv) and (Niter<self.Nitermax) ):\n",
    "            Niter = Niter+1\n",
    "            y = 1/(1+np.exp(-X.dot(w)))\n",
    "            R = np.diag(y*(1-y))  \n",
    "            w_old = w \n",
    "            try:\n",
    "                z = X.dot(w_old)-la.inv(R).dot(y-t) \n",
    "            except:\n",
    "                R = np.diag(y*(1-y) + self.cst)\n",
    "                z = X.dot(w_old)-la.inv(R).dot(y-t) \n",
    "            w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "            \n",
    "        self.w = w\n",
    "        self.Niter = Niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_early_stop = Logistique_Regression_early_stop(x_train_brut, class_train,  x_test_brut, class_test)\n",
    "rl_early_stop.preprocessing()\n",
    "rl_early_stop.fit()\n",
    "rl_early_stop.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_early_stop.Niter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got Niter=Nitermax so we did not use the early stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = rl_early_stop.confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_early_stop = Logistique_Regression_early_stop(x_train_brut, class_train,  x_test_brut, class_test, Nitermax=200)\n",
    "rl_early_stop.preprocessing()\n",
    "rl_early_stop.fit()\n",
    "rl_early_stop.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_early_stop.Niter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there is an inversion problem at the 15th iteration !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = rl_early_stop.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a better result, which makes sense since we did two more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1,3, figsize=(15, 15))\n",
    "\n",
    "# Exemple 1\n",
    "ax[0].set_title(\"Example 1\")\n",
    "ax[0].imshow(np.reshape(rl_early_stop.test[np.where(rl_early_stop.t_test_pred-rl_early_stop.class_test != 0)[0][0]], (16,16)))\n",
    "\n",
    "# Exemple 2\n",
    "ax[1].set_title(\"Example 2\")\n",
    "ax[1].imshow(np.reshape(rl_early_stop.test[np.where(rl_early_stop.t_test_pred-rl_early_stop.class_test != 0)[0][1]], (16,16)))\n",
    "\n",
    "# Exemple 3\n",
    "ax[2].set_title(\"Example 3\")\n",
    "ax[2].imshow(np.reshape(rl_early_stop.test[np.where(rl_early_stop.t_test_pred-rl_early_stop.class_test != 0)[0][-1]], (16,16)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two examples have been classified as 6 instead of 5 and the last example has been classified as 5 instead of 6. We notice that for the first two examples, the high bar of the 5 extends a lot and thus a non-negligible number of important pixels of the image are not taken into account when passing from x_test_brut to x_test (i.e. after reduction of the dataset by calculating the sigma on pixels). Since only a small number of pixels are kept, it is understandable that the classification is more erroneous than in the case where more information is kept with the raw dataset. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(0.01,0.901,0.01)\n",
    "false_positive = np.zeros(values.shape[0])\n",
    "false_negative = np.zeros(values.shape[0])\n",
    "number_of_iterations = np.zeros(values.shape[0])\n",
    "for i,value in enumerate(values):\n",
    "    rl_early_stop = Logistique_Regression_early_stop(x_train_brut, class_train,  x_test_brut, class_test, Nitermax=200)\n",
    "    rl_early_stop.preprocessing(value)\n",
    "    rl_early_stop.fit()\n",
    "    rl_early_stop.predict()\n",
    "    cm = rl_early_stop.confusion_matrix(show=False)\n",
    "    false_positive[i] = cm[0,1]\n",
    "    false_negative[i] = cm[1,0]\n",
    "    number_of_iterations[i] = rl_early_stop.Niter\n",
    "\n",
    "df_analyse = pd.DataFrame({\n",
    "    \"Value for preprocessing\": values,\n",
    "    \"Number of false positive\": false_positive,\n",
    "    \"Number of false negative\": false_negative,\n",
    "    \"Number of iterations\": number_of_iterations  \n",
    "})\n",
    "\n",
    "px.line(df_analyse, \"Value for preprocessing\",y= [\"Number of iterations\", \"Number of false positive\", \"Number of false negative\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our dataset, modifying the threshold for the pixels in the preprocessing allows to obtain better results. In any case, the estimator remains better with the 5 than with the 6 but changing the threshold to 0.52 allows to obtain optimal results on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the early stop method, we get good results. The estimator is however better at recognizing the 5 than the 6. We will compare this method to the added constant method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "cst_values = np.array([1e-15, 1e-14, 1e-13, 1e-12, 1e-11, 1e-10, 1e-9,1e-8, 1e-7,  1e-6, 1e-5, 1e-4, 1e-3,1e-2, 1e-1])\n",
    "false_positive = np.zeros(cst_values.shape[0])\n",
    "false_negative = np.zeros(cst_values.shape[0])\n",
    "number_of_iterations = np.zeros(cst_values.shape[0])\n",
    "for i,cst in enumerate(cst_values):\n",
    "    rl_cst = Logistique_Regression_with_constant(x_train_brut, class_train,  x_test_brut, class_test, cst=cst, Nitermax=200)\n",
    "    rl_cst.preprocessing()\n",
    "    rl_cst.fit()\n",
    "    rl_cst.predict()\n",
    "    cm = rl_cst.confusion_matrix(show=False)\n",
    "    false_positive[i] = cm[0,1]\n",
    "    false_negative[i] = cm[1,0]\n",
    "    number_of_iterations[i] = rl_cst.Niter\n",
    "\n",
    "df_analyse = pd.DataFrame({\n",
    "    \"Constant value\": cst_values,\n",
    "    \"Number of false positive\": false_positive,\n",
    "    \"Number of false negative\": false_negative,\n",
    "    \"Number of iterations\": number_of_iterations  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_analyse, \"Constant value\",y= [\"Number of iterations\", \"Number of false positive\", \"Number of false negative\"], log_x=True, title=\"Analysis as a function of the constant value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ You can click on Number of iterations to see only the variations of the number of false positives and false negatives ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of false positives and false negatives varies according to the value of the constant which shows that the value of the constant has a real impact on the training of the classifier. However, we manage to have a better result for a value of the constant equal to $10^{-7}$. However, the results remain similar and with the early stop technique the algorithm has a much shorter execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_cst = Logistique_Regression_with_constant(x_train_brut, class_train,  x_test_brut, class_test, cst=1e-7, Nitermax=200)\n",
    "rl_cst.preprocessing()\n",
    "rl_cst.fit()\n",
    "rl_cst.predict()\n",
    "cm = rl_cst.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see how much of an impact adding this constant to the R matrix has is to look at the algorithm output w as a function of the constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_cst_1 = Logistique_Regression_with_constant(x_train_brut, class_train,  x_test_brut, class_test, cst=1e-7, Nitermax=200)\n",
    "rl_cst_1.preprocessing()\n",
    "rl_cst_1.fit()\n",
    "rl_cst_2 = Logistique_Regression_with_constant(x_train_brut, class_train,  x_test_brut, class_test, cst=1e-4, Nitermax=200)\n",
    "rl_cst_2.preprocessing()\n",
    "rl_cst_2.fit()\n",
    "\n",
    "print(\"First values of w for cst=1e-7\\n\")\n",
    "print(rl_cst_1.w[0:5],\"\\n\")\n",
    "print(\"First values of w for cst=1e-4\\n\")\n",
    "print(rl_cst_2.w[0:5],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Logistic regression using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. **Go to** http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html for a presentation of the logistic regression model in `scikit-learn`.\n",
    "\n",
    "2. **Apply** it to the present data set.\n",
    "\n",
    "3. **Comment** on the use of logistic regression.\n",
    "\n",
    "*Indication : you may have a look at* \n",
    "\n",
    "a) Theory : http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html\n",
    "\n",
    "b) Video :  https://www.coursera.org/learn/machine-learning/lecture/4BHEy/regularized-logistic-regression \n",
    "\n",
    "c) Example : http://scikit-learn.org/stable/auto_examples/exercises/plot_digits_classification_exercise.html#sphx-glr-auto-examples-exercises-plot-digits-classification-exercise-py\n",
    "\n",
    "*for a short presentation of regularized logistic regression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "synth_train = np.loadtxt('synth_train.txt') \n",
    "class_train = synth_train[:,0]\n",
    "class_train_1 = np.where(synth_train[:,0]==1)[0]\n",
    "class_train_2 = np.where(synth_train[:,0]==2)[0]\n",
    "x_train = synth_train[:,1:]\n",
    "N_train = np.size(x_train,axis=0)\n",
    "\n",
    "# Test set\n",
    "synth_test = np.loadtxt('synth_test.txt')\n",
    "class_test = synth_test[:,0]\n",
    "class_test_1 = np.where(synth_test[:,0]==1)[0]\n",
    "class_test_2 = np.where(synth_test[:,0]==2)[0]\n",
    "x_test = synth_test[:,1:]\n",
    "N_test = np.size(x_test,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, fit_intercept=True, tol=1e-3).fit(x_train, class_train)\n",
    "w = clf.coef_[0]\n",
    "bias = clf.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Prédiction : {clf.predict(x_test)}\")\n",
    "print(f\"Score : {clf.score(x_test, class_test)*100}%\")\n",
    "print(f\"Coefficients de la régression : {bias, w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scikit-learn regression method, we find a regression function of the form :\n",
    "\n",
    "$y = 2.63*x[0] - 2.12*x[1] + 5.46$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate_train = 1 - clf.score(x_train, class_train)\n",
    "error_rate_test = 1 - clf.score(x_test, class_test)\n",
    "\n",
    "df_error_rate_comparison.loc['Logistic Regression with Sklearn'] = [error_rate_train, error_rate_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_error_rate_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain strange results on the error rate for the training dataset which does not seem to be better than the test one and which is worse than the one implemented at the beginning of the TP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, the algorithm converged before the max iteration number so assuming we have the same input parameters as for Sklearn, we should have the same results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the dataset of the second exercise :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_sig = np.where(np.std(x_train_brut, axis=0) > 0.5)[0].tolist()\n",
    "\n",
    "x_train_brut = np.vstack((train_5[:145,:], train_6[:200,:]))\n",
    "N_train = np.size(x_train_brut,axis=0)\n",
    "class_train = np.ones((345,1))   # label 1 for digit 6\n",
    "class_train[:145] = 0       # label 0 for digit 5\n",
    "class_train = np.squeeze(class_train)\n",
    "\n",
    "x_test_brut = np.vstack((train_5[145:,:], train_6[200:,:]))\n",
    "N_test = np.size(train_5,axis=0)+np.size(train_6,axis=0)-N_train\n",
    "class_test = np.ones((875,1))\n",
    "class_test[:(556-145)] = 0\n",
    "class_test = np.squeeze(class_test)\n",
    "\n",
    "lis_sig = np.where(np.std(x_train_brut, axis=0) > 0.5)[0].tolist()\n",
    "\n",
    "x_train = x_train_brut[:, lis_sig]\n",
    "x_test = x_test_brut[:, lis_sig]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, fit_intercept=True, tol=1e-3).fit(x_train_brut, class_train)\n",
    "w = clf.coef_[0]\n",
    "bias = clf.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Prédiction : {clf.predict(x_test)}\")\n",
    "print(f\"Score on brut dataset : {clf.score(x_test_brut, class_test)*100}%\")\n",
    "# print(f\"Coefficients de la régression : {bias, w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, fit_intercept=True,tol=1e-3).fit(x_train, class_train)\n",
    "w = clf.coef_[0]\n",
    "bias = clf.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Prédiction : {clf.predict(x_test)}\")\n",
    "print(f\"Score on dataset with lis_sig : {clf.score(x_test, class_test)*100}%\")\n",
    "# print(f\"Coefficients de la régression : {bias, w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that keeping specific pixels based on the standard deviation increases the score of the prediction !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, x_test, class_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0,tol=1e-5, solver='liblinear').fit(x_train, class_train)\n",
    "plot_confusion_matrix(clf, x_test, class_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get much better results with the sklearn implementation. Moreover, depending on the solver used, the results are different. In the sklearn implementation, we can choose several logistic regression implementation methods that will not converge to the same solution, as we can see with the different confusion matrices above.\n",
    "\n",
    "The algorithm seen in class for logistic regression is much less efficient than all the algorithms used by sklearn.\n",
    "\n",
    "According to the Sklearn implementation, some methods are faster than other and more useful in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "rl_cst = Logistique_Regression_with_constant(x_train_brut, class_train,  x_test_brut, class_test, cst=1e-7, Nitermax=100)\n",
    "rl_early_stop = Logistique_Regression_early_stop(x_train_brut, class_train,  x_test_brut, class_test, Nitermax=100)\n",
    "rl_early_stop.preprocessing()\n",
    "rl_cst.preprocessing()\n",
    "loop = 10\n",
    "execution_time_early_stop = timeit.timeit(lambda: rl_early_stop.fit(), number=loop) / loop\n",
    "execution_time_cst= timeit.timeit(lambda: rl_cst.fit(), number=loop) / loop\n",
    "execution_time_sklearn = timeit.timeit(lambda: LogisticRegression(random_state=0,tol=1e-3).fit(x_train, class_train), number=loop) / loop\n",
    "\n",
    "print(f\"The sklearn implementation is {int(execution_time_early_stop / execution_time_sklearn)} faster than our implementation with early stop.\\nThe sklearn implementation is {int(execution_time_cst / execution_time_sklearn)} faster than our implementation with constant.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
