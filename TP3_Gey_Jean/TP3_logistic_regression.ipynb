{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP3 : Logistic regression\n",
    "\n",
    "The purpose of this tutorial is to implement and use the Logistic Regression for binary classification. We will apply this\n",
    "method to the problem of handwritten characters to learn how to\n",
    "distinguish two numbers (here 5 and 6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic regression, IRLS algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary question: the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have a look at the function `regression_logistique.m` and locate the main steps of the algorithm you have been taught (see course).\n",
    "You can comment the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_logistique(X,t,Nitermax=20,eps_conv=1e-3):\n",
    "    '''Entrees :\n",
    "    X = [ones(N_train,1) x_train];\n",
    "    t = class_train \n",
    "    Nitermax = nombre maximale d'itérations (20 par défaut)\n",
    "    eps_conv = critère de convergence sur norm(w-w_old)/norm(w) ; \n",
    "    eps_conv=1e-3 par défaut\n",
    "    \n",
    "    Sorties : \n",
    "    w : vecteur des coefficients de régression logistique\n",
    "   Niter : nombre d'itérations utilisées effectivement\n",
    "   \n",
    "   Fonction de régression logistique pour la classification binaire.\n",
    "   \n",
    "   Utilisation :\n",
    "       Nitermax = 50\n",
    "       eps_conv = 1e-4\n",
    "       [w,Niter] = regression_logistique(X,t,Nitermax,eps_conv)\n",
    "    '''\n",
    "    N_train = X.shape[0]\n",
    "\n",
    "    #initialisation : 1 pas de l'algorithme IRLS\n",
    "    w = np.zeros((X.shape[1],))\n",
    "    w_old = w \n",
    "    y = 1/2*np.ones((N_train,))\n",
    "    R = np.diag(y*(1-y))   # diag(y_n(1-y_n))\n",
    "    z = X.dot(w_old)-la.inv(R).dot(y-t)\n",
    "    w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "\n",
    "    # boucle appliquant l'algortihme de Newton-Raphson\n",
    "    Niter = 1\n",
    "    while ( (la.norm(w-w_old)/la.norm(w)>eps_conv) | (Niter<Nitermax) ):\n",
    "        Niter = Niter+1\n",
    "        y = 1/(1+np.exp(-X.dot(w)))\n",
    "        R = np.diag(y*(1-y))  \n",
    "        w_old = w \n",
    "        z = X.dot(w_old)-la.inv(R).dot(y-t) \n",
    "        w = la.inv(X.T.dot(R).dot(X)).dot(X.T).dot(R).dot(z)\n",
    "         \n",
    "    return w, Niter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading & preparing synthetic data\n",
    "\n",
    "Load the training and test data sets `synth_train.txt`\n",
    "and `synth_test.txt`. The targets t belong to {1,2} and the features  \n",
    "x belong to R^2. \n",
    "\n",
    "We have 100 training samples and 200 test samples\n",
    "\n",
    "* the 1st column contains the label of each sample, \n",
    "* columns 2 and 3 contain the coordinate of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "synth_train = np.loadtxt('synth_train.txt') \n",
    "class_train = synth_train[:,0]\n",
    "class_train_1 = np.where(synth_train[:,0]==1)[0]\n",
    "class_train_2 = np.where(synth_train[:,0]==2)[0]\n",
    "x_train = synth_train[:,1:]\n",
    "N_train = np.size(x_train,axis=0)\n",
    "\n",
    "# Test set\n",
    "synth_test = np.loadtxt('synth_test.txt')\n",
    "class_test = synth_test[:,0]\n",
    "class_test_1 = np.where(synth_test[:,0]==1)[0]\n",
    "class_test_2 = np.where(synth_test[:,0]==2)[0]\n",
    "x_test = synth_test[:,1:]\n",
    "N_test = np.size(x_test,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "px.scatter(pd.DataFrame(synth_train, columns = ['classe', 'x1', 'x2']),x='x1', y='x2', color='classe', title = 'Data visualization of training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(synth_test, columns = ['classe', 'x1', 'x2']),x='x1', y='x2', color='classe', title = 'Data visualization of test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing features for logistic regression (binary classification)\n",
    "First, we prepare the feature matrix and the target vector associated to \n",
    "the training and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((N_train,1)),x_train))\n",
    "t = 2-class_train   # 0 if class=2, 1 if class=1\n",
    "\n",
    "X_test = np.hstack((np.ones((N_test,1)),x_test))\n",
    "t_test = 2-class_test   # 0 if class=2, 1 if class=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 : the logistic function of decision\n",
    "\n",
    "1. Use the function `regression_logistique.m` to estimate the logistic\n",
    "regression vector `w`. *Indication : use `Nitermax = 50;\n",
    "eps_conv=1e-3;`.*\n",
    "2. Compute the decision function $f(x) = argmax_k P(C_k|x)$ on the test set\n",
    "to get the classification results. Recall that $y_n=\\sigma(w^T x)$ (logistic function)\n",
    "and that *using vectors* you may directly write $y=\\sigma(Xw)$, with the\n",
    "column of ones in X.\n",
    "3. Display the results by plotting the points from both the training set\n",
    "and the test set.\n",
    "4. Write the equation which defines the decision boundary.\n",
    "5. Artificially add a few points to the training set far from the decision boundary to check the robustness of logistic regression to outliers. Check the behaviour of LDA for comparison in this case and comment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1\n",
    "w, Niter = regression_logistique(X, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lambda x: np.where(np.exp(x)/( 1 + np.exp(x))<=0.5, 0 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_pred = predictions(X @ w)\n",
    "t_test_pred = predictions(X_test @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(prediction=2-t_train_pred, x1 = synth_train[:,1], x2 = synth_train[:,2])),x='x1', y='x2', color='prediction', title = 'Data visualization of predictions for training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(prediction=2-t_test_pred, x1 = synth_test[:,1], x2 = synth_test[:,2])),x='x1', y='x2', color='prediction', title = 'Data visualization of predictions for training dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can draw the decision boundary $w^Tx = 0$ by using: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# First compute w... then: \n",
    "x1 = np.linspace(-2.5,1.5,10) \n",
    "x2 = (-w[0]-w[1]*x1)/w[2]\n",
    "fig1 = px.line(pd.DataFrame(dict(x1=x1, x2=x2)), x='x1', y='x2')\n",
    "fig2 = px.scatter(pd.DataFrame(dict(prediction=2-t_train_pred, x1 = synth_train[:,1], x2 = synth_train[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary = go.Figure(data = fig1.data + fig2.data)\n",
    "fig_boundary.update_layout(dict(title='Decision Boundary with predictions of training dataset'))\n",
    "fig_boundary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2_test = px.scatter(pd.DataFrame(dict(prediction=2-t_test_pred, x1 = synth_test[:,1], x2 = synth_test[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary_test = go.Figure(data = fig1.data + fig2_test.data)\n",
    "fig_boundary_test.update_layout(dict(title='Decision Boundary with predictions of test dataset'))\n",
    "fig_boundary_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding artificial outliers : \n",
    "\n",
    "Il ne faut pas ajouter des points trop éloignés des points du dataset sinon la matrice R dans l'implementation de l'algorithme ne sera plus inversible : Pourquoi -> Pour de très grosses valeur on a un sigma très proche de 0 ou de 1 et donc y-t très proche de zero donc plus inversible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "nb_points = 5\n",
    "outliers_points = np.repeat([[-3,4],[2,-1]],[nb_points,nb_points], axis=0) + norm.rvs(loc = 0, scale = 0.2, size = nb_points*4).reshape((nb_points*2,2))\n",
    "classe_outliers = np.concatenate((np.full(nb_points,1), np.full(nb_points,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(pd.DataFrame(dict(x1=outliers_points[:,0], x2=outliers_points[:,1], classe=classe_outliers)),\n",
    "           x='x1',\n",
    "           y='x2',\n",
    "           color='classe',\n",
    "           title = 'Outliers who where be add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = np.hstack((np.ones((outliers_points.shape[0],1)),outliers_points))\n",
    "t_outliers = 2-classe_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_outliers, Niter_outliers = regression_logistique(np.concatenate((X, X_outliers)), np.concatenate((t, t_outliers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train_outliers_pred = predictions(np.concatenate((X, X_outliers)) @ w_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_outliers = (-w_outliers[0]-w_outliers[1]*x1)/w_outliers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1_outliers = px.line(pd.DataFrame(dict(x1=x1, x2=x2)), x='x1', y='x2')\n",
    "fig2_outliers = px.scatter(pd.DataFrame(dict(prediction=2-t_train_outliers_pred, x1 = np.concatenate((X, X_outliers))[:,1], x2 = np.concatenate((X, X_outliers))[:,2])),x='x1', y='x2', color='prediction')\n",
    "fig_boundary_outliers = go.Figure(data = fig1_outliers.data + fig2_outliers.data)\n",
    "fig_boundary_outliers.update_layout(dict(title='Decision Boundary with predictions of training dataset'))\n",
    "fig_boundary_outliers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Application: handwritten digits recognition 5 & 6\n",
    "We load 2 matrices which contain each a sequence of examples of 16x16 images \n",
    "of handwritten digits which are 5 and 6 here. Each line of the matrix\n",
    "contains 256 pixel values coding for the gray level of a 16x16 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_5 = np.loadtxt('train_5.txt',delimiter=',')   # 556 samples\n",
    "train_6 = np.loadtxt('train_6.txt',delimiter=',')   # 664 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 5\n",
    "n=9\n",
    "I = np.reshape(train_5[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digit 6\n",
    "n=5\n",
    "I = reshape(train_6[n,:],(16,16))\n",
    "\n",
    "plt.imshow(I,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the training and test sets\n",
    "\n",
    "We keep in the training set the 145 first images of 5s and the 200 first\n",
    "images of 6s:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_brut = np.vstack((train_5[:145,:], train_6[:200,:]))\n",
    "N_train = np.size(x_train_brut,axis=0)\n",
    "class_train = np.ones((345,1))   # label 1 for digit 6\n",
    "class_train[:145] = 0       # label 0 for digit 5\n",
    "\n",
    "x_test_brut = np.vstack((train_5[145:,:], train_6[200:,:]))\n",
    "N_test = np.size(train_5,axis=0)+np.size(train_6,axis=0)-N_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: logistic regression to classify 5 & 6\n",
    "\n",
    "1. Note that pixel values are between -1 and 1 by using the functions\n",
    " `min(I(:))` and `max(I(:))`.\n",
    "2. Identify the indices of the most significant pixels, which are defined \n",
    "as having a standard deviation greater than 0.5 here. We denote by `lis_sig`\n",
    "the list of positions of these significant pixels in the image vector.\n",
    "_Indication : the function `std` gives the standard deviation (columnwise\n",
    "in matrices) and you should find 173 pixel positions.\n",
    "3. Show a binary image to locate these pixels.\n",
    "_Indication : `Isig = zeros(16); Isig(list_sig)=1; Isig=Isig';`._\n",
    "4. Define the training set `x_train` from `x_train_brut` from the significant pixels only.\n",
    "5. Do the same with `x_test_brut` to extract `x_test`.\n",
    "6. Use `regression_logistique.m` to estimate the logistic regression vector\n",
    "`w` from the training set `x_train`. \n",
    "Choose `Nitermax = 13; eps_conv = 1e-3;`\n",
    "7. Compute the decision function and the labels of the test set `x_test`. \n",
    "_Indication : do not forget the column of ones !_\n",
    "8. Estimate the classification error rate by using :\n",
    "`erreur = sum(abs(class-class_test))/N_test;`.\n",
    "9. Locate some misclassified examples and visualize the corresponding image.\n",
    "Comment on your results and observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sig = np.where(np.std(x_train_brut, axis=0) > 0.5)[0].tolist()\n",
    "\n",
    "Isig = np.zeros(256)\n",
    "Isig[list_sig]=1\n",
    "Isig = np.reshape(np.array(Isig), (16,16))\n",
    "plt.imshow(Isig, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Logistic regression using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. **Go to** http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html for a presentation of the logistic regression model in `scikit-learn`.\n",
    "\n",
    "2. **Apply** it to the present data set.\n",
    "\n",
    "3. **Comment** on the use of logistic regression.\n",
    "\n",
    "*Indication : you may have a look at* \n",
    "\n",
    "a) Theory : http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html\n",
    "\n",
    "b) Video :  https://www.coursera.org/learn/machine-learning/lecture/4BHEy/regularized-logistic-regression \n",
    "\n",
    "c) Example : http://scikit-learn.org/stable/auto_examples/exercises/plot_digits_classification_exercise.html#sphx-glr-auto-examples-exercises-plot-digits-classification-exercise-py\n",
    "\n",
    "*for a short presentation of regularized logistic regression.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=True).fit(x_train, class_train)\n",
    "w = clf.coef_[0]\n",
    "bias = clf.intercept_[0]\n",
    "print(f\"Prédiction : {clf.predict(x_test)}\")\n",
    "print(f\"Score : {clf.score(x_test, class_test)*100}%\")\n",
    "print(f\"Coefficients de la régression : {bias, w}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à la méthode de régression par scikit-learn, on trouve une fonction de régression de la forme :\n",
    "\n",
    "$y = 2.63*x[0] - 2.12*x[1] + 5.46$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f35fc4c302fa05141946eeee87a02543093a5f9fe4b255be016c8b1114de3b55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
